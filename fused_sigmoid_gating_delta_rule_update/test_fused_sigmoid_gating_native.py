#!/usr/bin/env python3
"""
æµ‹è¯• Fused Sigmoid Gating Delta Rule Update Native å®ç°

éªŒè¯ PyTorch native å®ç°ä¸ Triton kernel çš„ç­‰ä»·æ€§
"""

from fused_sigmoid_gating_native_implementation import (
    fused_sigmoid_gating_delta_rule_update_native,
    fused_sigmoid_gating_delta_rule_update_native_optimized,
)
import torch
import sys
import os
import argparse
from pathlib import Path

# ã€ä¿®å¤å¯¼å…¥é—®é¢˜ã€‘æ·»åŠ  python ç›®å½•åˆ°æœç´¢è·¯å¾„
project_root = Path(__file__).parent
python_dir = project_root / 'python'
if str(python_dir) not in sys.path:
    sys.path.insert(0, str(python_dir))
    print(f"âœ… å·²æ·»åŠ åˆ° Python è·¯å¾„: {python_dir}")

# å¯¼å…¥ native å®ç°

# å°è¯•å¯¼å…¥ Triton å®ç°
try:
    from sglang.srt.layers.attention.fla.fused_sigmoid_gating_recurrent import (
        fused_sigmoid_gating_delta_rule_update as fused_sigmoid_gating_delta_rule_update_triton,
    )
    HAS_TRITON = True
    print("âœ… Triton å®ç°å·²æˆåŠŸåŠ è½½")
except ImportError as e:
    HAS_TRITON = False
    print(f"âš ï¸  Triton å®ç°æœªæ‰¾åˆ°: {e}")
    print("    ä»…æµ‹è¯• native å®ç°çš„å†…éƒ¨ä¸€è‡´æ€§")

# ============================================================================
# å…¨å±€é…ç½®
# ============================================================================
DEVICE = None  # å°†åœ¨ parse_args ä¸­è®¾ç½®
SKIP_TRITON = False  # æ˜¯å¦è·³è¿‡ Triton å¯¹æ¯”


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='æµ‹è¯• Fused Sigmoid Gating Native å®ç°')
    parser.add_argument('--device', type=str, default='cpu',
                        help='æµ‹è¯•è®¾å¤‡ (cpu, cuda, cuda:0, etc.). é»˜è®¤: cpu')
    parser.add_argument('--skip-triton', action='store_true',
                        help='è·³è¿‡ Triton å¯¹æ¯”æµ‹è¯•')
    args = parser.parse_args()

    global DEVICE, SKIP_TRITON
    DEVICE = args.device
    SKIP_TRITON = args.skip_triton

    return args


def allclose_with_info(a, b, rtol=1e-3, atol=1e-4, name=""):
    """æ£€æŸ¥ä¸¤ä¸ª tensor æ˜¯å¦æ¥è¿‘ï¼Œå¹¶æ‰“å°è¯¦ç»†ä¿¡æ¯"""
    if a is None and b is None:
        return True
    if a is None or b is None:
        print(f"  âŒ {name}: ä¸€ä¸ªä¸º None")
        return False

    close = torch.allclose(a, b, rtol=rtol, atol=atol)

    if close:
        max_diff = (a - b).abs().max().item()
        print(f"  âœ… {name}: æœ€å¤§å·®å¼‚ {max_diff:.2e}")
    else:
        max_diff = (a - b).abs().max().item()
        mean_diff = (a - b).abs().mean().item()
        print(f"  âŒ {name}: æœ€å¤§å·®å¼‚ {max_diff:.2e}, å¹³å‡å·®å¼‚ {mean_diff:.2e}")
        print(f"     a: mean={a.mean():.6f}, std={a.std():.6f}")
        print(f"     b: mean={b.mean():.6f}, std={b.std():.6f}")

        # æ‰“å°ä¸åŒ¹é…çš„ä½ç½®ï¼ˆå‰å‡ ä¸ªï¼‰
        diff = (a - b).abs()
        max_indices = torch.topk(diff.flatten(), k=min(5, diff.numel()))[1]
        print(f"     Top 5 å·®å¼‚ä½ç½®:")
        for idx in max_indices:
            flat_idx = idx.item()
            multi_idx = torch.unravel_index(torch.tensor(flat_idx), a.shape)
            a_val = a.flatten()[flat_idx].item()
            b_val = b.flatten()[flat_idx].item()
            print(
                f"       ä½ç½® {tuple(x.item() for x in multi_idx)}: a={a_val:.6f}, b={b_val:.6f}, diff={abs(a_val-b_val):.6f}")

    return close


def test_case_1_basic_functionality():
    """æµ‹è¯• 1: åŸºæœ¬åŠŸèƒ½ï¼ˆå°è§„æ¨¡ï¼‰"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 1: åŸºæœ¬åŠŸèƒ½ï¼ˆå°è§„æ¨¡ï¼‰")
    print("=" * 80)

    # å‡†å¤‡æ•°æ®
    B, T, H, K, V = 2, 4, 2, 8, 8
    HV = H

    device = 'cuda:0'
    dtype = torch.float32

    print(f"é…ç½®: B={B}, T={T}, H={H}, HV={HV}, K={K}, V={V}")

    # åˆ›å»ºè¾“å…¥
    A_log = torch.randn(HV, dtype=dtype, device=device) * 0.1
    a = torch.randn(B, T, HV, dtype=dtype, device=device) * 0.1
    dt_bias = torch.randn(HV, dtype=dtype, device=device) * 0.1
    q = torch.randn(B, T, H, K, dtype=dtype, device=device) * 0.1
    k = torch.randn(B, T, H, K, dtype=dtype, device=device) * 0.1
    v = torch.randn(B, T, HV, V, dtype=dtype, device=device) * 0.1
    b = torch.randn(B, T, HV, dtype=dtype, device=device) * 0.1

    softplus_beta = 1.0
    softplus_threshold = 20.0

    print("\næµ‹è¯• 1.1: æ— åˆå§‹çŠ¶æ€")

    # Native å®ç°
    out_native = fused_sigmoid_gating_delta_rule_update_native(
        A_log, a, dt_bias, softplus_beta, softplus_threshold,
        q, k, v, b,
        initial_state_source=None,
        initial_state_indices=None,
        scale=None,
        use_qk_l2norm_in_kernel=False,
        cu_seqlens=None,
    )

    print(
        f"Native è¾“å‡º: shape={out_native.shape}, mean={out_native.mean():.6f}, std={out_native.std():.6f}")

    # ä¼˜åŒ–ç‰ˆæœ¬
    out_optimized = fused_sigmoid_gating_delta_rule_update_native_optimized(
        A_log, a, dt_bias, softplus_beta, softplus_threshold,
        q.clone(), k.clone(), v.clone(), b,
        initial_state_source=None,
        initial_state_indices=None,
        scale=None,
        use_qk_l2norm_in_kernel=False,
        cu_seqlens=None,
    )

    print(
        f"Optimized è¾“å‡º: shape={out_optimized.shape}, mean={out_optimized.mean():.6f}, std={out_optimized.std():.6f}")

    success = allclose_with_info(
        out_native, out_optimized, name="Native vs Optimized")

    if HAS_TRITON and device == 'cuda:0':
        # ä¸ Triton å¯¹æ¯”
        out_triton = fused_sigmoid_gating_delta_rule_update_triton(
            A_log, a, dt_bias, softplus_beta, softplus_threshold,
            q.clone(), k.clone(), v.clone(), b,
            initial_state_source=None,
            initial_state_indices=None,
            scale=None,
            use_qk_l2norm_in_kernel=False,
            cu_seqlens=None,
        )

        print(
            f"Triton è¾“å‡º: shape={out_triton.shape}, mean={out_triton.mean():.6f}, std={out_triton.std():.6f}")
        success &= allclose_with_info(
            out_native, out_triton, name="Native vs Triton")

    return success


def test_case_2_with_initial_state():
    """æµ‹è¯• 2: å¸¦åˆå§‹çŠ¶æ€"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 2: å¸¦åˆå§‹çŠ¶æ€")
    print("=" * 80)

    B, T, H, K, V = 2, 4, 2, 8, 8
    HV = H
    num_states = 3

    device = 'cpu'
    dtype = torch.float32

    print(
        f"é…ç½®: B={B}, T={T}, H={H}, HV={HV}, K={K}, V={V}, num_states={num_states}")

    # åˆ›å»ºè¾“å…¥
    A_log = torch.randn(HV, dtype=dtype, device=device) * 0.1
    a = torch.randn(B, T, HV, dtype=dtype, device=device) * 0.1
    dt_bias = torch.randn(HV, dtype=dtype, device=device) * 0.1
    q = torch.randn(B, T, H, K, dtype=dtype, device=device) * 0.1
    k = torch.randn(B, T, H, K, dtype=dtype, device=device) * 0.1
    v = torch.randn(B, T, HV, V, dtype=dtype, device=device) * 0.1
    b = torch.randn(B, T, HV, dtype=dtype, device=device) * 0.1

    # åˆå§‹çŠ¶æ€
    initial_state_source = torch.randn(
        num_states, HV, K, V, dtype=dtype, device=device) * 0.1
    initial_state_indices = torch.tensor(
        [0, 1], dtype=torch.long, device=device)  # ä½¿ç”¨çŠ¶æ€ 0 å’Œ 1

    softplus_beta = 1.0
    softplus_threshold = 20.0

    # Native å®ç°ï¼ˆå¤åˆ¶åˆå§‹çŠ¶æ€ä»¥é¿å…ä¿®æ”¹ï¼‰
    initial_state_source_native = initial_state_source.clone()
    out_native = fused_sigmoid_gating_delta_rule_update_native(
        A_log, a, dt_bias, softplus_beta, softplus_threshold,
        q, k, v, b,
        initial_state_source=initial_state_source_native,
        initial_state_indices=initial_state_indices,
        scale=None,
        use_qk_l2norm_in_kernel=False,
        cu_seqlens=None,
    )

    print(
        f"Native è¾“å‡º: shape={out_native.shape}, mean={out_native.mean():.6f}, std={out_native.std():.6f}")
    print(
        f"Native æœ€ç»ˆçŠ¶æ€: mean={initial_state_source_native.mean():.6f}, std={initial_state_source_native.std():.6f}")

    # ä¼˜åŒ–ç‰ˆæœ¬
    initial_state_source_optimized = initial_state_source.clone()
    out_optimized = fused_sigmoid_gating_delta_rule_update_native_optimized(
        A_log, a, dt_bias, softplus_beta, softplus_threshold,
        q.clone(), k.clone(), v.clone(), b,
        initial_state_source=initial_state_source_optimized,
        initial_state_indices=initial_state_indices,
        scale=None,
        use_qk_l2norm_in_kernel=False,
        cu_seqlens=None,
    )

    print(
        f"Optimized è¾“å‡º: shape={out_optimized.shape}, mean={out_optimized.mean():.6f}, std={out_optimized.std():.6f}")
    print(
        f"Optimized æœ€ç»ˆçŠ¶æ€: mean={initial_state_source_optimized.mean():.6f}, std={initial_state_source_optimized.std():.6f}")

    success = allclose_with_info(
        out_native, out_optimized, name="è¾“å‡º (Native vs Optimized)")
    success &= allclose_with_info(
        initial_state_source_native, initial_state_source_optimized, name="æœ€ç»ˆçŠ¶æ€ (Native vs Optimized)")

    return success


def test_case_3_with_l2norm():
    """æµ‹è¯• 3: å¸¦ L2 å½’ä¸€åŒ–"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 3: å¸¦ L2 å½’ä¸€åŒ–")
    print("=" * 80)

    B, T, H, K, V = 2, 4, 2, 8, 8
    HV = H

    device = 'cpu'
    dtype = torch.float32

    print(f"é…ç½®: B={B}, T={T}, H={H}, HV={HV}, K={K}, V={V}, use_qk_l2norm=True")

    # åˆ›å»ºè¾“å…¥
    A_log = torch.randn(HV, dtype=dtype, device=device) * 0.1
    a = torch.randn(B, T, HV, dtype=dtype, device=device) * 0.1
    dt_bias = torch.randn(HV, dtype=dtype, device=device) * 0.1
    q = torch.randn(B, T, H, K, dtype=dtype, device=device) * 0.1
    k = torch.randn(B, T, H, K, dtype=dtype, device=device) * 0.1
    v = torch.randn(B, T, HV, V, dtype=dtype, device=device) * 0.1
    b = torch.randn(B, T, HV, dtype=dtype, device=device) * 0.1

    softplus_beta = 1.0
    softplus_threshold = 20.0

    # Native å®ç°
    out_native = fused_sigmoid_gating_delta_rule_update_native(
        A_log, a, dt_bias, softplus_beta, softplus_threshold,
        q, k, v, b,
        initial_state_source=None,
        initial_state_indices=None,
        scale=None,
        use_qk_l2norm_in_kernel=True,  # å¼€å¯ L2 å½’ä¸€åŒ–
        cu_seqlens=None,
    )

    print(
        f"Native è¾“å‡º: shape={out_native.shape}, mean={out_native.mean():.6f}, std={out_native.std():.6f}")

    # ä¼˜åŒ–ç‰ˆæœ¬
    out_optimized = fused_sigmoid_gating_delta_rule_update_native_optimized(
        A_log, a, dt_bias, softplus_beta, softplus_threshold,
        q.clone(), k.clone(), v.clone(), b,
        initial_state_source=None,
        initial_state_indices=None,
        scale=None,
        use_qk_l2norm_in_kernel=True,
        cu_seqlens=None,
    )

    print(
        f"Optimized è¾“å‡º: shape={out_optimized.shape}, mean={out_optimized.mean():.6f}, std={out_optimized.std():.6f}")

    success = allclose_with_info(
        out_native, out_optimized, name="Native vs Optimized")

    return success


def test_case_4_custom_scale():
    """æµ‹è¯• 4: è‡ªå®šä¹‰ scale"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 4: è‡ªå®šä¹‰ scale")
    print("=" * 80)

    B, T, H, K, V = 2, 4, 2, 8, 8
    HV = H
    custom_scale = 0.5

    device = 'cpu'
    dtype = torch.float32

    print(
        f"é…ç½®: B={B}, T={T}, H={H}, HV={HV}, K={K}, V={V}, scale={custom_scale}")

    # åˆ›å»ºè¾“å…¥
    A_log = torch.randn(HV, dtype=dtype, device=device) * 0.1
    a = torch.randn(B, T, HV, dtype=dtype, device=device) * 0.1
    dt_bias = torch.randn(HV, dtype=dtype, device=device) * 0.1
    q = torch.randn(B, T, H, K, dtype=dtype, device=device) * 0.1
    k = torch.randn(B, T, H, K, dtype=dtype, device=device) * 0.1
    v = torch.randn(B, T, HV, V, dtype=dtype, device=device) * 0.1
    b = torch.randn(B, T, HV, dtype=dtype, device=device) * 0.1

    softplus_beta = 1.0
    softplus_threshold = 20.0

    # Native å®ç°
    out_native = fused_sigmoid_gating_delta_rule_update_native(
        A_log, a, dt_bias, softplus_beta, softplus_threshold,
        q, k, v, b,
        initial_state_source=None,
        initial_state_indices=None,
        scale=custom_scale,
        use_qk_l2norm_in_kernel=False,
        cu_seqlens=None,
    )

    print(
        f"Native è¾“å‡º: shape={out_native.shape}, mean={out_native.mean():.6f}, std={out_native.std():.6f}")

    # ä¼˜åŒ–ç‰ˆæœ¬
    out_optimized = fused_sigmoid_gating_delta_rule_update_native_optimized(
        A_log, a, dt_bias, softplus_beta, softplus_threshold,
        q.clone(), k.clone(), v.clone(), b,
        initial_state_source=None,
        initial_state_indices=None,
        scale=custom_scale,
        use_qk_l2norm_in_kernel=False,
        cu_seqlens=None,
    )

    print(
        f"Optimized è¾“å‡º: shape={out_optimized.shape}, mean={out_optimized.mean():.6f}, std={out_optimized.std():.6f}")

    success = allclose_with_info(
        out_native, out_optimized, name="Native vs Optimized")

    return success


def test_case_5_larger_scale():
    """æµ‹è¯• 5: è¾ƒå¤§è§„æ¨¡"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 5: è¾ƒå¤§è§„æ¨¡")
    print("=" * 80)

    B, T, H, K, V = 4, 8, 4, 16, 16
    HV = H

    device = 'cpu'
    dtype = torch.float32

    print(f"é…ç½®: B={B}, T={T}, H={H}, HV={HV}, K={K}, V={V}")

    # åˆ›å»ºè¾“å…¥
    A_log = torch.randn(HV, dtype=dtype, device=device) * 0.1
    a = torch.randn(B, T, HV, dtype=dtype, device=device) * 0.1
    dt_bias = torch.randn(HV, dtype=dtype, device=device) * 0.1
    q = torch.randn(B, T, H, K, dtype=dtype, device=device) * 0.1
    k = torch.randn(B, T, H, K, dtype=dtype, device=device) * 0.1
    v = torch.randn(B, T, HV, V, dtype=dtype, device=device) * 0.1
    b = torch.randn(B, T, HV, dtype=dtype, device=device) * 0.1

    softplus_beta = 1.0
    softplus_threshold = 20.0

    # Native å®ç°
    import time
    start = time.time()
    out_native = fused_sigmoid_gating_delta_rule_update_native(
        A_log, a, dt_bias, softplus_beta, softplus_threshold,
        q, k, v, b,
        initial_state_source=None,
        initial_state_indices=None,
        scale=None,
        use_qk_l2norm_in_kernel=False,
        cu_seqlens=None,
    )
    native_time = time.time() - start

    print(
        f"Native è¾“å‡º: shape={out_native.shape}, mean={out_native.mean():.6f}, std={out_native.std():.6f}, time={native_time:.3f}s")

    # ä¼˜åŒ–ç‰ˆæœ¬
    start = time.time()
    out_optimized = fused_sigmoid_gating_delta_rule_update_native_optimized(
        A_log, a, dt_bias, softplus_beta, softplus_threshold,
        q.clone(), k.clone(), v.clone(), b,
        initial_state_source=None,
        initial_state_indices=None,
        scale=None,
        use_qk_l2norm_in_kernel=False,
        cu_seqlens=None,
    )
    optimized_time = time.time() - start

    print(
        f"Optimized è¾“å‡º: shape={out_optimized.shape}, mean={out_optimized.mean():.6f}, std={out_optimized.std():.6f}, time={optimized_time:.3f}s")
    print(f"åŠ é€Ÿæ¯”: {native_time / optimized_time:.2f}x")

    success = allclose_with_info(
        out_native, out_optimized, name="Native vs Optimized")

    return success


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "â•”" + "=" * 78 + "â•—")
    print("â•‘" + " " * 10 +
          "Fused Sigmoid Gating Delta Rule Update æµ‹è¯•å¥—ä»¶" + " " * 20 + "â•‘")
    print("â•š" + "=" * 78 + "â•")

    print(f"\nâœ… PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"âœ… è®¾å¤‡: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
    print(f"âœ… Triton å®ç°: {'å¯ç”¨' if HAS_TRITON else 'ä¸å¯ç”¨ï¼ˆä»…æµ‹è¯• nativeï¼‰'}")

    results = []

    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    tests = [
        ("åŸºæœ¬åŠŸèƒ½", test_case_1_basic_functionality),
        ("å¸¦åˆå§‹çŠ¶æ€", test_case_2_with_initial_state),
        ("å¸¦ L2 å½’ä¸€åŒ–", test_case_3_with_l2norm),
        ("è‡ªå®šä¹‰ scale", test_case_4_custom_scale),
        ("è¾ƒå¤§è§„æ¨¡", test_case_5_larger_scale),
    ]

    for test_name, test_func in tests:
        try:
            success = test_func()
            results.append((test_name, success))
        except Exception as e:
            print(f"\n  âŒ æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            results.append((test_name, False))

    # æ€»ç»“
    print("\n" + "=" * 80)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 80)

    for test_name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{test_name:30s}: {status}")

    all_passed = all(success for _, success in results)

    if all_passed:
        print("\n" + "ğŸ‰ " * 20)
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Native å®ç°ä¸ Triton kernel å®Œå…¨ç­‰ä»·")
        print("ğŸ‰ " * 20)
    else:
        print("\n" + "âš ï¸  " * 20)
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°")
        print("âš ï¸  " * 20)

    return all_passed


if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)
