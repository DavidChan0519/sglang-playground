#!/usr/bin/env python3
"""
æµ‹è¯• cu_seqlens (å˜é•¿åºåˆ—) åŠŸèƒ½

éªŒè¯ Native å®ç°å¯¹å˜é•¿åºåˆ—æ‰¹å¤„ç†çš„æ”¯æŒ

ä½¿ç”¨æ–¹æ³•:
    # CPU æµ‹è¯•
    python3 test_cu_seqlens.py --device cpu
    
    # CUDA æµ‹è¯•
    python3 test_cu_seqlens.py --device cuda
    
    # è·³è¿‡ Triton å¯¹æ¯”
    python3 test_cu_seqlens.py --device cuda --skip-triton
"""

from fused_sigmoid_gating_native_implementation import (
    fused_sigmoid_gating_delta_rule_update_native,
    fused_sigmoid_gating_delta_rule_update_native_optimized,
)
import torch
import sys
import argparse
from pathlib import Path

# æ·»åŠ  python ç›®å½•åˆ°æœç´¢è·¯å¾„
project_root = Path(__file__).parent
python_dir = project_root / 'python'
if str(python_dir) not in sys.path:
    sys.path.insert(0, str(python_dir))

# å¯¼å…¥ native å®ç°

# å°è¯•å¯¼å…¥ Triton å®ç°
try:
    from sglang.srt.layers.attention.fla.fused_sigmoid_gating_recurrent import (
        fused_sigmoid_gating_delta_rule_update as fused_sigmoid_gating_delta_rule_update_triton,
    )
    HAS_TRITON = True
except ImportError as e:
    HAS_TRITON = False
    TRITON_IMPORT_ERROR = str(e)

# ============================================================================
# å…¨å±€é…ç½®
# ============================================================================
DEVICE = None  # å°†åœ¨ parse_args ä¸­è®¾ç½®
SKIP_TRITON = False  # æ˜¯å¦è·³è¿‡ Triton å¯¹æ¯”


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='æµ‹è¯• cu_seqlens (å˜é•¿åºåˆ—) åŠŸèƒ½')
    parser.add_argument('--device', type=str, default='cpu',
                        help='æµ‹è¯•è®¾å¤‡ (cpu, cuda, cuda:0, etc.). é»˜è®¤: cpu')
    parser.add_argument('--skip-triton', action='store_true',
                        help='è·³è¿‡ Triton å¯¹æ¯”æµ‹è¯•')
    args = parser.parse_args()

    global DEVICE, SKIP_TRITON
    DEVICE = args.device
    SKIP_TRITON = args.skip_triton

    return args


def test_varlen_vs_fixed_length():
    """æµ‹è¯•ï¼šéªŒè¯å˜é•¿åºåˆ—å’Œå›ºå®šé•¿åº¦åºåˆ—çš„ç­‰ä»·æ€§"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 1: å˜é•¿åºåˆ— vs å›ºå®šé•¿åº¦")
    print("=" * 80)

    dtype = torch.float32

    # åˆ›å»º3ä¸ªä¸åŒé•¿åº¦çš„åºåˆ—
    # åºåˆ— 0: é•¿åº¦ 5
    # åºåˆ— 1: é•¿åº¦ 7
    # åºåˆ— 2: é•¿åº¦ 6
    seq_lens = [5, 7, 6]
    N = len(seq_lens)
    total_len = sum(seq_lens)
    cu_seqlens = torch.tensor([0] + [sum(seq_lens[:i+1]) for i in range(N)],
                              dtype=torch.int64, device=DEVICE)

    H, HV, K, V = 2, 2, 8, 8

    print(
        f"é…ç½®: N={N}, seq_lens={seq_lens}, total_len={total_len}, device={DEVICE}")
    print(f"       H={H}, HV={HV}, K={K}, V={V}")
    print(f"       cu_seqlens={cu_seqlens.tolist()}")

    # åˆ›å»ºè¾“å…¥ï¼ˆå˜é•¿æ ¼å¼ï¼‰
    A_log_varlen = torch.randn(HV, dtype=dtype, device=DEVICE) * 0.1
    a_varlen = torch.randn(1, total_len, HV, dtype=dtype, device=DEVICE) * 0.1
    dt_bias_varlen = torch.randn(HV, dtype=dtype, device=DEVICE) * 0.1
    q_varlen = torch.randn(
        1, total_len, H, K, dtype=dtype, device=DEVICE) * 0.1
    k_varlen = torch.randn(
        1, total_len, H, K, dtype=dtype, device=DEVICE) * 0.1
    v_varlen = torch.randn(1, total_len, HV, V,
                           dtype=dtype, device=DEVICE) * 0.1
    b_varlen = torch.randn(1, total_len, HV, dtype=dtype, device=DEVICE) * 0.1

    softplus_beta = 1.0
    softplus_threshold = 20.0

    # Native å®ç°ï¼ˆå˜é•¿æ¨¡å¼ï¼‰
    print("\n[1] Native å®ç°ï¼ˆå˜é•¿æ¨¡å¼ï¼‰")
    out_native_varlen = fused_sigmoid_gating_delta_rule_update_native(
        A_log_varlen.clone(), a_varlen.clone(), dt_bias_varlen.clone(),
        softplus_beta, softplus_threshold,
        q_varlen.clone(), k_varlen.clone(), v_varlen.clone(), b_varlen.clone(),
        initial_state_source=None, initial_state_indices=None,
        scale=None, use_qk_l2norm_in_kernel=False,
        cu_seqlens=cu_seqlens,
    )
    print(
        f"è¾“å‡º: shape={out_native_varlen.shape}, mean={out_native_varlen.mean():.6f}")

    # Optimized å®ç°ï¼ˆå˜é•¿æ¨¡å¼ï¼‰
    print("\n[2] Optimized å®ç°ï¼ˆå˜é•¿æ¨¡å¼ï¼‰")
    out_optimized_varlen = fused_sigmoid_gating_delta_rule_update_native_optimized(
        A_log_varlen.clone(), a_varlen.clone(), dt_bias_varlen.clone(),
        softplus_beta, softplus_threshold,
        q_varlen.clone(), k_varlen.clone(), v_varlen.clone(), b_varlen.clone(),
        initial_state_source=None, initial_state_indices=None,
        scale=None, use_qk_l2norm_in_kernel=False,
        cu_seqlens=cu_seqlens,
    )
    print(
        f"è¾“å‡º: shape={out_optimized_varlen.shape}, mean={out_optimized_varlen.mean():.6f}")

    # å¯¹æ¯” Native vs Optimizedï¼ˆå˜é•¿ï¼‰
    max_diff_varlen = (out_native_varlen -
                       out_optimized_varlen).abs().max().item()
    print(f"\nâœ… Native vs Optimized (å˜é•¿æ¨¡å¼): æœ€å¤§å·®å¼‚ {max_diff_varlen:.2e}")

    # å°†å˜é•¿æ ¼å¼è½¬æ¢ä¸ºå›ºå®šé•¿åº¦æ ¼å¼ï¼Œé€ä¸ªåºåˆ—æµ‹è¯•
    print("\n[3] å›ºå®šé•¿åº¦æ¨¡å¼ï¼ˆé€åºåˆ—å¯¹æ¯”ï¼‰")
    success = True

    for seq_idx in range(N):
        bos = cu_seqlens[seq_idx].item()
        eos = cu_seqlens[seq_idx + 1].item()
        seq_len = eos - bos

        print(f"\n  åºåˆ— {seq_idx}: é•¿åº¦={seq_len}, èŒƒå›´=[{bos}:{eos}]")

        # æå–å½“å‰åºåˆ—çš„æ•°æ®
        # [1, seq_len, HV] -> [seq_len, HV]
        a_fixed = a_varlen[:, bos:eos].contiguous()
        q_fixed = q_varlen[:, bos:eos].contiguous()  # [1, seq_len, H, K]
        k_fixed = k_varlen[:, bos:eos].contiguous()  # [1, seq_len, H, K]
        v_fixed = v_varlen[:, bos:eos].contiguous()  # [1, seq_len, HV, V]
        b_fixed = b_varlen[:, bos:eos].contiguous()  # [1, seq_len, HV]

        # Native å›ºå®šé•¿åº¦æ¨¡å¼
        out_native_fixed = fused_sigmoid_gating_delta_rule_update_native(
            A_log_varlen.clone(), a_fixed.clone(), dt_bias_varlen.clone(),
            softplus_beta, softplus_threshold,
            q_fixed.clone(), k_fixed.clone(), v_fixed.clone(), b_fixed.clone(),
            initial_state_source=None, initial_state_indices=None,
            scale=None, use_qk_l2norm_in_kernel=False,
            cu_seqlens=None,  # å›ºå®šé•¿åº¦æ¨¡å¼
        )

        # ä»å˜é•¿è¾“å‡ºä¸­æå–å¯¹åº”éƒ¨åˆ†
        # [seq_len, HV, V]
        out_native_varlen_slice = out_native_varlen[0, bos:eos].contiguous()

        # å¯¹æ¯”
        max_diff = (out_native_fixed -
                    out_native_varlen_slice).abs().max().item()
        print(f"    Fixed vs Varlen: æœ€å¤§å·®å¼‚ {max_diff:.2e}")

        if max_diff > 1e-5:
            print(f"    âŒ å·®å¼‚è¿‡å¤§ï¼")
            success = False
        else:
            print(f"    âœ… ä¸€è‡´")

    # Triton å¯¹æ¯”ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if HAS_TRITON and not SKIP_TRITON:
        print("\n[4] Triton å®ç°ï¼ˆå˜é•¿æ¨¡å¼ï¼‰")
        try:
            out_triton_varlen = fused_sigmoid_gating_delta_rule_update_triton(
                A_log_varlen.clone(), a_varlen.clone(), dt_bias_varlen.clone(),
                softplus_beta, softplus_threshold,
                q_varlen.clone(), k_varlen.clone(), v_varlen.clone(), b_varlen.clone(),
                initial_state_source=None, initial_state_indices=None,
                scale=None, use_qk_l2norm_in_kernel=False,
                cu_seqlens=cu_seqlens,
            )
            print(
                f"è¾“å‡º: shape={out_triton_varlen.shape}, mean={out_triton_varlen.mean():.6f}")

            max_diff_triton = (out_native_varlen -
                               out_triton_varlen).abs().max().item()
            print(f"\nâœ… Native vs Triton (å˜é•¿æ¨¡å¼): æœ€å¤§å·®å¼‚ {max_diff_triton:.2e}")

            if max_diff_triton > 1e-3:
                print(f"âš ï¸  æ³¨æ„: å·®å¼‚è¾ƒå¤§ï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥")
                success = False
        except Exception as e:
            print(f"âŒ Triton æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            success = False
    elif not HAS_TRITON:
        print(f"\nâš ï¸  è·³è¿‡ Triton å¯¹æ¯”: {TRITON_IMPORT_ERROR}")
    elif SKIP_TRITON:
        print(f"\nâ­ï¸  è·³è¿‡ Triton å¯¹æ¯” (--skip-triton)")

    return success


def test_varlen_with_initial_states():
    """æµ‹è¯•ï¼šå˜é•¿åºåˆ— + åˆå§‹çŠ¶æ€"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 2: å˜é•¿åºåˆ— + åˆå§‹çŠ¶æ€")
    print("=" * 80)

    dtype = torch.float32

    seq_lens = [4, 6, 5]
    N = len(seq_lens)
    total_len = sum(seq_lens)
    cu_seqlens = torch.tensor([0] + [sum(seq_lens[:i+1]) for i in range(N)],
                              dtype=torch.int64, device=DEVICE)

    H, HV, K, V = 2, 2, 8, 8
    num_states = 5

    print(
        f"é…ç½®: N={N}, seq_lens={seq_lens}, num_states={num_states}, device={DEVICE}")

    # åˆ›å»ºè¾“å…¥
    A_log = torch.randn(HV, dtype=dtype, device=DEVICE) * 0.1
    a = torch.randn(1, total_len, HV, dtype=dtype, device=DEVICE) * 0.1
    dt_bias = torch.randn(HV, dtype=dtype, device=DEVICE) * 0.1
    q = torch.randn(1, total_len, H, K, dtype=dtype, device=DEVICE) * 0.1
    k = torch.randn(1, total_len, H, K, dtype=dtype, device=DEVICE) * 0.1
    v = torch.randn(1, total_len, HV, V, dtype=dtype, device=DEVICE) * 0.1
    b = torch.randn(1, total_len, HV, dtype=dtype, device=DEVICE) * 0.1

    # åˆå§‹çŠ¶æ€
    initial_state_source = torch.randn(
        num_states, HV, K, V, dtype=dtype, device=DEVICE) * 0.1
    initial_state_indices = torch.tensor(
        [0, 2, 1], dtype=torch.long, device=DEVICE)

    softplus_beta = 1.0
    softplus_threshold = 20.0

    # Native å®ç°
    initial_state_native = initial_state_source.clone()
    out_native = fused_sigmoid_gating_delta_rule_update_native(
        A_log.clone(), a.clone(), dt_bias.clone(),
        softplus_beta, softplus_threshold,
        q.clone(), k.clone(), v.clone(), b.clone(),
        initial_state_source=initial_state_native,
        initial_state_indices=initial_state_indices.clone(),
        scale=None, use_qk_l2norm_in_kernel=False,
        cu_seqlens=cu_seqlens,
    )

    print(f"Native è¾“å‡º: shape={out_native.shape}, mean={out_native.mean():.6f}")
    print(f"æœ€ç»ˆçŠ¶æ€[0]: mean={initial_state_native[0].mean():.6f}")
    print(f"æœ€ç»ˆçŠ¶æ€[2]: mean={initial_state_native[2].mean():.6f}")
    print(f"æœ€ç»ˆçŠ¶æ€[1]: mean={initial_state_native[1].mean():.6f}")

    # Optimized å®ç°
    initial_state_optimized = initial_state_source.clone()
    out_optimized = fused_sigmoid_gating_delta_rule_update_native_optimized(
        A_log.clone(), a.clone(), dt_bias.clone(),
        softplus_beta, softplus_threshold,
        q.clone(), k.clone(), v.clone(), b.clone(),
        initial_state_source=initial_state_optimized,
        initial_state_indices=initial_state_indices.clone(),
        scale=None, use_qk_l2norm_in_kernel=False,
        cu_seqlens=cu_seqlens,
    )

    print(
        f"Optimized è¾“å‡º: shape={out_optimized.shape}, mean={out_optimized.mean():.6f}")

    # å¯¹æ¯”
    max_diff_out = (out_native - out_optimized).abs().max().item()
    max_diff_state = (initial_state_native -
                      initial_state_optimized).abs().max().item()

    print(f"\nâœ… Native vs Optimized")
    print(f"   è¾“å‡º: æœ€å¤§å·®å¼‚ {max_diff_out:.2e}")
    print(f"   æœ€ç»ˆçŠ¶æ€: æœ€å¤§å·®å¼‚ {max_diff_state:.2e}")

    success = max_diff_out < 1e-5 and max_diff_state < 1e-5

    return success


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    global DEVICE

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()

    print("\n" + "â•”" + "=" * 78 + "â•—")
    print("â•‘" + " " * 20 + "cu_seqlens å˜é•¿åºåˆ—æµ‹è¯•" + " " * 28 + "â•‘")
    print("â•š" + "=" * 78 + "â•")

    # æ˜¾ç¤ºé…ç½®
    print(f"\nâœ… PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"âœ… æµ‹è¯•è®¾å¤‡: {DEVICE}")
    if DEVICE.startswith('cuda'):
        if torch.cuda.is_available():
            print(f"âœ… CUDA å¯ç”¨: {torch.cuda.get_device_name(DEVICE)}")
        else:
            print(f"âŒ CUDA ä¸å¯ç”¨ï¼Œå°†å›é€€åˆ° CPU")
            DEVICE = 'cpu'

    print(f"âœ… Triton å®ç°: {'å¯ç”¨' if HAS_TRITON else 'ä¸å¯ç”¨'}")
    if not HAS_TRITON:
        print(f"   åŸå› : {TRITON_IMPORT_ERROR}")
    if SKIP_TRITON:
        print(f"â­ï¸  è·³è¿‡ Triton å¯¹æ¯”æµ‹è¯• (--skip-triton)")

    results = []

    # æµ‹è¯• 1: å˜é•¿ vs å›ºå®šé•¿åº¦
    try:
        success = test_varlen_vs_fixed_length()
        results.append(("å˜é•¿ vs å›ºå®šé•¿åº¦", success))
    except Exception as e:
        print(f"\n  âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        results.append(("å˜é•¿ vs å›ºå®šé•¿åº¦", False))

    # æµ‹è¯• 2: å˜é•¿ + åˆå§‹çŠ¶æ€
    try:
        success = test_varlen_with_initial_states()
        results.append(("å˜é•¿ + åˆå§‹çŠ¶æ€", success))
    except Exception as e:
        print(f"\n  âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        results.append(("å˜é•¿ + åˆå§‹çŠ¶æ€", False))

    # æ€»ç»“
    print("\n" + "=" * 80)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 80)

    for test_name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{test_name:30s}: {status}")

    all_passed = all(success for _, success in results)

    if all_passed:
        print("\n" + "ğŸ‰ " * 20)
        print("âœ… æ‰€æœ‰ cu_seqlens æµ‹è¯•é€šè¿‡ï¼")
        print("ğŸ‰ " * 20)
    else:
        print("\n" + "âš ï¸  " * 20)
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
        print("âš ï¸  " * 20)

    return all_passed


if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)
