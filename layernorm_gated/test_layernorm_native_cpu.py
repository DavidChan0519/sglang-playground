#!/usr/bin/env python3
"""
æµ‹è¯• PyTorch Native LayerNorm - å®Œæ•´æµ‹è¯•å¥—ä»¶

åŠŸèƒ½ï¼š
1. éªŒè¯ Native å®ç°çš„é€»è¾‘æ­£ç¡®æ€§
2. ä¸ PyTorch æ ‡å‡†å®ç°å¯¹æ¯”
3. ä¸ Triton å®ç°å¯¹æ¯”ï¼ˆå¦‚æœå¯ç”¨ï¼‰
4. æ”¯æŒç»Ÿä¸€çš„ device é…ç½®

ä½¿ç”¨æ–¹æ³•:
    # CPU æµ‹è¯•ï¼ˆé»˜è®¤ï¼‰
    python3 test_layernorm_native_cpu.py --device cpu
    
    # CUDA æµ‹è¯•
    python3 test_layernorm_native_cpu.py --device cuda
    
    # è·³è¿‡ Triton å¯¹æ¯”
    python3 test_layernorm_native_cpu.py --device cuda --skip-triton
"""

from layernorm_native_implementation import (
    _layer_norm_fwd_native,
    layernorm_fn_native,
    rmsnorm_fn_native,
    simple_layernorm_native,
)
import torch
import sys
import argparse
from pathlib import Path

# æ·»åŠ  python ç›®å½•åˆ°æœç´¢è·¯å¾„
project_root = Path(__file__).parent
python_dir = project_root / 'python'
if str(python_dir) not in sys.path:
    sys.path.insert(0, str(python_dir))

# å¯¼å…¥ native å®ç°

# å°è¯•å¯¼å…¥ Triton å®ç°
try:
    from sglang.srt.layers.attention.fla.layernorm_gated import (
        _layer_norm_fwd as _layer_norm_fwd_triton,
    )
    HAS_TRITON = True
except ImportError as e:
    HAS_TRITON = False
    TRITON_IMPORT_ERROR = str(e)

# ============================================================================
# å…¨å±€é…ç½®
# ============================================================================
DEVICE = None  # å°†åœ¨ parse_args ä¸­è®¾ç½®
SKIP_TRITON = False  # æ˜¯å¦è·³è¿‡ Triton å¯¹æ¯”


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='LayerNorm Native å®ç°æµ‹è¯•å¥—ä»¶')
    parser.add_argument('--device', type=str, default='cpu',
                        help='æµ‹è¯•è®¾å¤‡ (cpu, cuda, cuda:0, etc.). é»˜è®¤: cpu')
    parser.add_argument('--skip-triton', action='store_true',
                        help='è·³è¿‡ Triton å¯¹æ¯”æµ‹è¯•')
    args = parser.parse_args()

    global DEVICE, SKIP_TRITON
    DEVICE = args.device
    SKIP_TRITON = args.skip_triton

    return args


def allclose_with_info(a, b, rtol=1e-4, atol=1e-5, name=""):
    """æ£€æŸ¥ä¸¤ä¸ª tensor æ˜¯å¦æ¥è¿‘ï¼Œå¹¶æ‰“å°è¯¦ç»†ä¿¡æ¯"""
    if a is None and b is None:
        return True
    if a is None or b is None:
        print(f"  âŒ {name}: ä¸€ä¸ªä¸º None")
        return False

    close = torch.allclose(a, b, rtol=rtol, atol=atol)

    if close:
        max_diff = (a - b).abs().max().item()
        print(f"  âœ… {name}: æœ€å¤§å·®å¼‚ {max_diff:.2e}")
    else:
        max_diff = (a - b).abs().max().item()
        mean_diff = (a - b).abs().mean().item()
        print(f"  âŒ {name}: æœ€å¤§å·®å¼‚ {max_diff:.2e}, å¹³å‡å·®å¼‚ {mean_diff:.2e}")
        print(
            f"     a: mean={a.mean():.6f}, std={a.std():.6f}, min={a.min():.6f}, max={a.max():.6f}")
        print(
            f"     b: mean={b.mean():.6f}, std={b.std():.6f}, min={b.min():.6f}, max={b.max():.6f}")

    return close


def test_with_triton(test_name, native_out, x, weight, bias, eps, z=None,
                     group_size=None, norm_before_gate=True, is_rms_norm=False):
    """ç»Ÿä¸€çš„ Triton å¯¹æ¯”æµ‹è¯•"""
    if not HAS_TRITON:
        print(f"  âš ï¸  è·³è¿‡ Triton å¯¹æ¯”: {TRITON_IMPORT_ERROR}")
        return True

    if SKIP_TRITON:
        print(f"  â­ï¸  è·³è¿‡ Triton å¯¹æ¯” (--skip-triton)")
        return True

    if not DEVICE.startswith('cuda'):
        print(f"  â­ï¸  è·³è¿‡ Triton å¯¹æ¯” (Triton éœ€è¦ CUDA)")
        return True

    try:
        # å°†æ•°æ®ç§»åˆ° CUDAï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
        x_cuda = x.cuda() if not x.is_cuda else x
        weight_cuda = weight.cuda() if not weight.is_cuda else weight
        bias_cuda = bias.cuda() if not bias.is_cuda else bias
        z_cuda = z.cuda() if z is not None and not z.is_cuda else z

        # è°ƒç”¨ Triton å®ç°
        out_triton, mean_triton, rstd_triton = _layer_norm_fwd_triton(
            x_cuda.clone(), weight_cuda, bias_cuda, eps,
            z=z_cuda,
            out=None,
            group_size=group_size,
            norm_before_gate=norm_before_gate,
            is_rms_norm=is_rms_norm,
        )

        # å°†ç»“æœç§»å›åŸè®¾å¤‡è¿›è¡Œå¯¹æ¯”
        out_triton = out_triton.cpu() if native_out.device.type == 'cpu' else out_triton

        print(
            f"  Triton è¾“å‡º: shape={out_triton.shape}, mean={out_triton.mean():.6f}, std={out_triton.std():.6f}")

        success = allclose_with_info(
            native_out, out_triton, name="Native vs Triton", rtol=1e-3, atol=1e-4)
        return success

    except Exception as e:
        print(f"  âŒ Triton æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_basic_functionality():
    """æµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 1: åŸºæœ¬åŠŸèƒ½æµ‹è¯•")
    print("=" * 80)

    # å‡†å¤‡æ•°æ®
    M, N = 8, 32
    x = torch.randn(M, N, dtype=torch.float32, device=DEVICE)
    weight = torch.randn(N, dtype=torch.float32, device=DEVICE)
    bias = torch.randn(N, dtype=torch.float32, device=DEVICE)
    eps = 1e-5

    print(
        f"è¾“å…¥: x={x.shape}, weight={weight.shape}, bias={bias.shape}, device={DEVICE}")

    # Native å®ç°
    out_native, mean_native, rstd_native = _layer_norm_fwd_native(
        x, weight, bias, eps,
        z=None,
        group_size=None,
        norm_before_gate=True,
        is_rms_norm=False,
    )

    print(
        f"Native è¾“å‡º: out={out_native.shape}, mean={mean_native.shape}, rstd={rstd_native.shape}")

    # ä¸ PyTorch æ ‡å‡†å®ç°å¯¹æ¯”
    layer_norm = torch.nn.LayerNorm(N, eps=eps, device=DEVICE)
    layer_norm.weight.data = weight.clone()
    layer_norm.bias.data = bias.clone()
    out_torch = layer_norm(x)

    success = allclose_with_info(
        out_native, out_torch, name="Native vs PyTorch LayerNorm")

    # éªŒè¯ç»Ÿè®¡é‡
    expected_mean = x.mean(dim=1)  # [M]
    expected_var = x.var(dim=1, unbiased=False)  # [M]
    expected_rstd = 1.0 / torch.sqrt(expected_var + eps)

    print(f"\néªŒè¯ç»Ÿè®¡é‡:")
    allclose_with_info(mean_native, expected_mean, name="  mean")
    allclose_with_info(rstd_native, expected_rstd, name="  rstd")

    # ä¸ Triton å¯¹æ¯”
    print(f"\nä¸ Triton å¯¹æ¯”:")
    success &= test_with_triton(
        "åŸºæœ¬åŠŸèƒ½", out_native, x, weight, bias, eps,
        z=None, group_size=None, norm_before_gate=True, is_rms_norm=False
    )

    return success


def test_rmsnorm():
    """æµ‹è¯• RMSNorm"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 2: RMSNorm")
    print("=" * 80)

    M, N = 16, 64
    x = torch.randn(M, N, dtype=torch.float32, device=DEVICE)
    weight = torch.ones(N, dtype=torch.float32, device=DEVICE)
    bias = torch.zeros(N, dtype=torch.float32, device=DEVICE)
    eps = 1e-6

    print(f"è¾“å…¥: x={x.shape}, device={DEVICE}")

    # Native å®ç°
    out_native, mean_native, rstd_native = _layer_norm_fwd_native(
        x, weight, bias, eps,
        z=None,
        group_size=None,
        norm_before_gate=True,
        is_rms_norm=True,
    )

    print(f"Native è¾“å‡º: out={out_native.shape}")
    print(f"  mean={mean_native} (should be None)")
    print(f"  rstd={rstd_native.shape}")

    assert mean_native is None, "RMSNorm should not compute mean"

    # æ‰‹åŠ¨éªŒè¯ RMSNorm
    rms = torch.sqrt((x ** 2).mean(dim=1, keepdim=True) + eps)
    expected_out = (x / rms) * weight + bias

    success = allclose_with_info(
        out_native, expected_out, name="Native vs Manual RMSNorm")

    # ä¸ Triton å¯¹æ¯”
    print(f"\nä¸ Triton å¯¹æ¯”:")
    success &= test_with_triton(
        "RMSNorm", out_native, x, weight, bias, eps,
        z=None, group_size=None, norm_before_gate=True, is_rms_norm=True
    )

    return success


def test_gating():
    """æµ‹è¯• Gating (SwiGLU)"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 3: Gating (SwiGLU)")
    print("=" * 80)

    M, N = 8, 32
    x = torch.randn(M, N, dtype=torch.float32, device=DEVICE)
    weight = torch.ones(N, dtype=torch.float32, device=DEVICE)
    bias = torch.zeros(N, dtype=torch.float32, device=DEVICE)
    z = torch.randn(M, N, dtype=torch.float32, device=DEVICE)
    eps = 1e-5

    print(f"è¾“å…¥: x={x.shape}, z={z.shape}, device={DEVICE}")

    # æµ‹è¯• 1: Gating BEFORE norm
    print("\n  æµ‹è¯• 3.1: Gating BEFORE Normalization")
    out_before, _, _ = _layer_norm_fwd_native(
        x.clone(), weight, bias, eps,
        z=z,
        group_size=None,
        norm_before_gate=False,  # BEFORE
        is_rms_norm=False,
    )

    # æ‰‹åŠ¨éªŒè¯
    x_gated = x * z * torch.sigmoid(z)
    mean = x_gated.mean(dim=1, keepdim=True)
    var = x_gated.var(dim=1, unbiased=False, keepdim=True)
    x_norm = (x_gated - mean) / torch.sqrt(var + eps)
    expected_before = x_norm * weight + bias

    success1 = allclose_with_info(
        out_before, expected_before, name="  Gating BEFORE")

    # ä¸ Triton å¯¹æ¯”
    print(f"\n  ä¸ Triton å¯¹æ¯” (BEFORE):")
    success1 &= test_with_triton(
        "Gating BEFORE", out_before, x, weight, bias, eps,
        z=z, group_size=None, norm_before_gate=False, is_rms_norm=False
    )

    # æµ‹è¯• 2: Gating AFTER norm
    print("\n  æµ‹è¯• 3.2: Gating AFTER Normalization")
    out_after, _, _ = _layer_norm_fwd_native(
        x.clone(), weight, bias, eps,
        z=z,
        group_size=None,
        norm_before_gate=True,  # AFTER
        is_rms_norm=False,
    )

    # æ‰‹åŠ¨éªŒè¯
    mean = x.mean(dim=1, keepdim=True)
    var = x.var(dim=1, unbiased=False, keepdim=True)
    x_norm = (x - mean) / torch.sqrt(var + eps)
    y_norm = x_norm * weight + bias
    expected_after = y_norm * z * torch.sigmoid(z)

    success2 = allclose_with_info(
        out_after, expected_after, name="  Gating AFTER")

    # ä¸ Triton å¯¹æ¯”
    print(f"\n  ä¸ Triton å¯¹æ¯” (AFTER):")
    success2 &= test_with_triton(
        "Gating AFTER", out_after, x, weight, bias, eps,
        z=z, group_size=None, norm_before_gate=True, is_rms_norm=False
    )

    return success1 and success2


def test_groupnorm():
    """æµ‹è¯• GroupNorm"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 4: GroupNorm")
    print("=" * 80)

    M, N = 8, 96
    group_size = 32  # 3 groups
    x = torch.randn(M, N, dtype=torch.float32, device=DEVICE)
    weight = torch.randn(N, dtype=torch.float32, device=DEVICE)
    bias = torch.randn(N, dtype=torch.float32, device=DEVICE)
    eps = 1e-5

    print(
        f"è¾“å…¥: x={x.shape}, group_size={group_size} ({N // group_size} groups), device={DEVICE}")

    # Native å®ç°
    out_native, mean_native, rstd_native = _layer_norm_fwd_native(
        x, weight, bias, eps,
        z=None,
        group_size=group_size,
        norm_before_gate=True,
        is_rms_norm=False,
    )

    print(f"Native è¾“å‡º: out={out_native.shape}")
    print(f"  mean={mean_native.shape} (should be [ngroups * M])")
    print(f"  rstd={rstd_native.shape}")

    ngroups = N // group_size
    assert mean_native.numel() == ngroups * \
        M, f"Expected {ngroups * M} elements, got {mean_native.numel()}"

    # æ‰‹åŠ¨éªŒè¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
    x_reshaped = x.view(M, ngroups, group_size)
    mean_manual = x_reshaped.mean(dim=2)  # [M, ngroups]
    var_manual = x_reshaped.var(dim=2, unbiased=False)  # [M, ngroups]
    rstd_manual = 1.0 / torch.sqrt(var_manual + eps)

    x_norm = (x_reshaped - mean_manual.unsqueeze(2)) * rstd_manual.unsqueeze(2)

    weight_reshaped = weight.view(ngroups, group_size)
    bias_reshaped = bias.view(ngroups, group_size)

    y_manual = x_norm * \
        weight_reshaped.unsqueeze(0) + bias_reshaped.unsqueeze(0)
    y_manual = y_manual.view(M, N)

    success = allclose_with_info(
        out_native, y_manual, name="Native vs Manual GroupNorm")

    # ä¸ Triton å¯¹æ¯”
    print(f"\nä¸ Triton å¯¹æ¯”:")
    success &= test_with_triton(
        "GroupNorm", out_native, x, weight, bias, eps,
        z=None, group_size=group_size, norm_before_gate=True, is_rms_norm=False
    )

    return success


def test_high_level_api():
    """æµ‹è¯•é«˜å±‚ API"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 5: é«˜å±‚ API")
    print("=" * 80)

    batch_size, seq_len, hidden_dim = 2, 16, 64
    x = torch.randn(batch_size, seq_len, hidden_dim,
                    dtype=torch.float32, device=DEVICE)
    weight = torch.randn(hidden_dim, dtype=torch.float32, device=DEVICE)
    bias = torch.randn(hidden_dim, dtype=torch.float32, device=DEVICE)
    eps = 1e-5

    print(f"è¾“å…¥: x={x.shape} (3D), device={DEVICE}")

    # LayerNorm API
    print("\n  æµ‹è¯• layernorm_fn_native:")
    out_ln = layernorm_fn_native(x, weight, bias, eps=eps)
    print(f"    è¾“å‡º: {out_ln.shape}")
    assert out_ln.shape == x.shape, f"Shape mismatch: {out_ln.shape} vs {x.shape}"

    # ä¸ PyTorch å¯¹æ¯”
    layer_norm = torch.nn.LayerNorm(hidden_dim, eps=eps, device=DEVICE)
    layer_norm.weight.data = weight.clone()
    layer_norm.bias.data = bias.clone()
    out_torch = layer_norm(x)

    success1 = allclose_with_info(
        out_ln, out_torch, name="    LayerNorm vs PyTorch")

    # RMSNorm API
    print("\n  æµ‹è¯• rmsnorm_fn_native:")
    out_rms = rmsnorm_fn_native(x, weight, bias, eps=eps)
    print(f"    è¾“å‡º: {out_rms.shape}")
    assert out_rms.shape == x.shape

    # ç®€å•éªŒè¯
    print(f"    è¾“å‡ºç»Ÿè®¡: mean={out_rms.mean():.6f}, std={out_rms.std():.6f}")
    success2 = True  # No direct comparison

    # æ³¨æ„ï¼šé«˜å±‚ API çš„ Triton å¯¹æ¯”æ¯”è¾ƒå¤æ‚ï¼Œå› ä¸ºéœ€è¦å±•å¹³è¾“å…¥
    # è¿™é‡Œæˆ‘ä»¬ä¸»è¦éªŒè¯ä¸ PyTorch çš„ä¸€è‡´æ€§
    print(f"\n  â­ï¸  é«˜å±‚ API ä¸»è¦éªŒè¯ä¸ PyTorch çš„ä¸€è‡´æ€§")

    return success1 and success2


def test_simple_layernorm():
    """æµ‹è¯•ç®€åŒ–ç‰ˆ LayerNorm"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 6: ç®€åŒ–ç‰ˆ LayerNorm")
    print("=" * 80)

    M, N = 8, 32
    x = torch.randn(M, N, dtype=torch.float32, device=DEVICE)
    weight = torch.randn(N, dtype=torch.float32, device=DEVICE)
    bias = torch.randn(N, dtype=torch.float32, device=DEVICE)
    eps = 1e-5

    print(f"è¾“å…¥: x={x.shape}, device={DEVICE}")

    # ç®€åŒ–ç‰ˆå®ç°
    out_simple = simple_layernorm_native(x, weight, bias, eps=eps)

    # ä¸ PyTorch å¯¹æ¯”
    layer_norm = torch.nn.LayerNorm(N, eps=eps, device=DEVICE)
    layer_norm.weight.data = weight.clone()
    layer_norm.bias.data = bias.clone()
    out_torch = layer_norm(x)

    success = allclose_with_info(
        out_simple, out_torch, name="Simple vs PyTorch")

    # ä¸ Triton å¯¹æ¯”
    print(f"\nä¸ Triton å¯¹æ¯”:")
    success &= test_with_triton(
        "ç®€åŒ–ç‰ˆ LayerNorm", out_simple, x, weight, bias, eps,
        z=None, group_size=None, norm_before_gate=True, is_rms_norm=False
    )

    return success


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    global DEVICE

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()

    print("\n" + "â•”" + "=" * 78 + "â•—")
    print("â•‘" + " " * 15 + "LayerNorm Native å®ç°æµ‹è¯•å¥—ä»¶" + " " * 28 + "â•‘")
    print("â•š" + "=" * 78 + "â•")

    # æ˜¾ç¤ºé…ç½®
    print(f"\nâœ… PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"âœ… æµ‹è¯•è®¾å¤‡: {DEVICE}")
    if DEVICE.startswith('cuda'):
        if torch.cuda.is_available():
            print(f"âœ… CUDA å¯ç”¨: {torch.cuda.get_device_name(DEVICE)}")
        else:
            print(f"âŒ CUDA ä¸å¯ç”¨ï¼Œå°†å›é€€åˆ° CPU")
            DEVICE = 'cpu'

    print(f"âœ… Triton å®ç°: {'å¯ç”¨' if HAS_TRITON else 'ä¸å¯ç”¨'}")
    if not HAS_TRITON:
        print(f"   åŸå› : {TRITON_IMPORT_ERROR}")
    if SKIP_TRITON:
        print(f"â­ï¸  è·³è¿‡ Triton å¯¹æ¯”æµ‹è¯• (--skip-triton)")

    results = []

    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    tests = [
        ("åŸºæœ¬åŠŸèƒ½", test_basic_functionality),
        ("RMSNorm", test_rmsnorm),
        ("Gating (SwiGLU)", test_gating),
        ("GroupNorm", test_groupnorm),
        ("é«˜å±‚ API", test_high_level_api),
        ("ç®€åŒ–ç‰ˆ LayerNorm", test_simple_layernorm),
    ]

    for test_name, test_func in tests:
        try:
            success = test_func()
            results.append((test_name, success))
        except Exception as e:
            print(f"\n  âŒ æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            results.append((test_name, False))

    # æ€»ç»“
    print("\n" + "=" * 80)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 80)

    for test_name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{test_name:30s}: {status}")

    all_passed = all(success for _, success in results)

    if all_passed:
        print("\n" + "ğŸ‰ " * 20)
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Native å®ç°ä¸ PyTorch/Triton å®Œå…¨ç­‰ä»·")
        print("ğŸ‰ " * 20)
    else:
        print("\n" + "âš ï¸  " * 20)
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°")
        print("âš ï¸  " * 20)

    return all_passed


if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)
