#!/usr/bin/env python3
"""
æµ‹è¯• PyTorch Native LayerNorm ä¸ Triton Kernel çš„ç­‰ä»·æ€§

æµ‹è¯•åœºæ™¯ï¼š
1. æ ‡å‡† LayerNorm
2. RMSNorm
3. LayerNorm + Gating (SwiGLU)
4. GroupNorm
5. å„ç§ç»„åˆ
"""

import torch
import sys

# å¯¼å…¥ native å®ç°
from layernorm_native_implementation import (
    _layer_norm_fwd_native,
    layernorm_fn_native,
    rmsnorm_fn_native,
    simple_layernorm_native,
)

# å°è¯•å¯¼å…¥ Triton å®ç°
try:
    from python.sglang.srt.layers.attention.fla.layernorm_gated import (
        _layer_norm_fwd as _layer_norm_fwd_triton,
        layernorm_fn as layernorm_fn_triton,
        rmsnorm_fn as rmsnorm_fn_triton,
    )
    HAS_TRITON = True
except ImportError:
    HAS_TRITON = False
    print("âš ï¸  Triton å®ç°æœªæ‰¾åˆ°ï¼Œä»…æµ‹è¯• native å®ç°çš„æ­£ç¡®æ€§")


def allclose_with_info(a, b, rtol=1e-4, atol=1e-5, name=""):
    """æ£€æŸ¥ä¸¤ä¸ª tensor æ˜¯å¦æ¥è¿‘ï¼Œå¹¶æ‰“å°è¯¦ç»†ä¿¡æ¯"""
    if a is None and b is None:
        return True
    if a is None or b is None:
        print(f"  âŒ {name}: ä¸€ä¸ªä¸º None")
        return False

    close = torch.allclose(a, b, rtol=rtol, atol=atol)

    if close:
        max_diff = (a - b).abs().max().item()
        print(f"  âœ… {name}: æœ€å¤§å·®å¼‚ {max_diff:.2e}")
    else:
        max_diff = (a - b).abs().max().item()
        mean_diff = (a - b).abs().mean().item()
        print(f"  âŒ {name}: æœ€å¤§å·®å¼‚ {max_diff:.2e}, å¹³å‡å·®å¼‚ {mean_diff:.2e}")
        print(f"     a: mean={a.mean():.6f}, std={a.std():.6f}")
        print(f"     b: mean={b.mean():.6f}, std={b.std():.6f}")

    return close


def test_case_1_standard_layernorm():
    """æµ‹è¯• 1: æ ‡å‡† LayerNormï¼ˆæ— é¢å¤–åŠŸèƒ½ï¼‰"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 1: æ ‡å‡† LayerNorm")
    print("=" * 80)

    # å‡†å¤‡æ•°æ®
    M, N = 32, 256
    x = torch.randn(M, N, dtype=torch.float32, device='cuda')
    weight = torch.randn(N, dtype=torch.float32, device='cuda')
    bias = torch.randn(N, dtype=torch.float32, device='cuda')
    eps = 1e-5

    print(f"è¾“å…¥: x={x.shape}, weight={weight.shape}, bias={bias.shape}")

    # Native å®ç°
    out_native, mean_native, rstd_native = _layer_norm_fwd_native(
        x, weight, bias, eps,
        z=None,
        group_size=None,
        norm_before_gate=True,
        is_rms_norm=False,
    )

    print(
        f"Native è¾“å‡º: out={out_native.shape}, mean={mean_native.shape}, rstd={rstd_native.shape}")

    # ä¸ PyTorch æ ‡å‡†å®ç°å¯¹æ¯”
    layer_norm = torch.nn.LayerNorm(N, eps=eps, device='cuda')
    layer_norm.weight.data = weight.clone()
    layer_norm.bias.data = bias.clone()
    out_torch = layer_norm(x)

    success = allclose_with_info(
        out_native, out_torch, name="vs PyTorch LayerNorm")

    if HAS_TRITON:
        # ä¸ Triton å®ç°å¯¹æ¯”
        out_triton, mean_triton, rstd_triton = _layer_norm_fwd_triton(
            x.clone(), weight.clone(), bias.clone(), eps,
            z=None,
            group_size=None,
            norm_before_gate=True,
            is_rms_norm=False,
        )

        print(
            f"Triton è¾“å‡º: out={out_triton.shape}, mean={mean_triton.shape}, rstd={rstd_triton.shape}")

        success &= allclose_with_info(
            out_native, out_triton, name="Native vs Triton (out)")
        success &= allclose_with_info(
            mean_native, mean_triton, name="Native vs Triton (mean)")
        success &= allclose_with_info(
            rstd_native, rstd_triton, name="Native vs Triton (rstd)")

    return success


def test_case_2_rmsnorm():
    """æµ‹è¯• 2: RMSNorm"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 2: RMSNorm")
    print("=" * 80)

    # å‡†å¤‡æ•°æ®
    M, N = 64, 512
    x = torch.randn(M, N, dtype=torch.bfloat16, device='cuda')
    weight = torch.randn(N, dtype=torch.bfloat16, device='cuda')
    bias = torch.randn(N, dtype=torch.bfloat16, device='cuda')
    eps = 1e-6

    print(f"è¾“å…¥: x={x.shape}, dtype={x.dtype}")

    # Native å®ç°
    out_native, mean_native, rstd_native = _layer_norm_fwd_native(
        x, weight, bias, eps,
        z=None,
        group_size=None,
        norm_before_gate=True,
        is_rms_norm=True,  # RMSNorm
    )

    print(
        f"Native è¾“å‡º: out={out_native.shape}, mean={mean_native}, rstd={rstd_native.shape}")
    assert mean_native is None, "RMSNorm should not compute mean"

    success = True

    if HAS_TRITON:
        # ä¸ Triton å®ç°å¯¹æ¯”
        out_triton, mean_triton, rstd_triton = _layer_norm_fwd_triton(
            x.clone(), weight.clone(), bias.clone(), eps,
            z=None,
            group_size=None,
            norm_before_gate=True,
            is_rms_norm=True,
        )

        success &= allclose_with_info(
            out_native, out_triton, rtol=1e-3, atol=1e-4, name="Native vs Triton (out)")
        success &= allclose_with_info(
            rstd_native, rstd_triton, rtol=1e-3, atol=1e-4, name="Native vs Triton (rstd)")
        assert mean_triton is None, "Triton RMSNorm should not compute mean"

    return success


def test_case_3_gating_before_norm():
    """æµ‹è¯• 3: Gating BEFORE Normalization"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 3: Gating (SwiGLU) BEFORE Normalization")
    print("=" * 80)

    # å‡†å¤‡æ•°æ®
    M, N = 16, 128
    x = torch.randn(M, N, dtype=torch.float32, device='cuda')
    weight = torch.randn(N, dtype=torch.float32, device='cuda')
    bias = torch.randn(N, dtype=torch.float32, device='cuda')
    z = torch.randn(M, N, dtype=torch.float32, device='cuda')
    eps = 1e-5

    print(f"è¾“å…¥: x={x.shape}, z={z.shape} (gating)")

    # Native å®ç°
    out_native, mean_native, rstd_native = _layer_norm_fwd_native(
        x, weight, bias, eps,
        z=z,
        group_size=None,
        norm_before_gate=False,  # Gating BEFORE norm
        is_rms_norm=False,
    )

    print(f"Native è¾“å‡º: out={out_native.shape}")

    success = True

    if HAS_TRITON:
        # ä¸ Triton å®ç°å¯¹æ¯”
        out_triton, mean_triton, rstd_triton = _layer_norm_fwd_triton(
            x.clone(), weight.clone(), bias.clone(), eps,
            z=z.clone(),
            group_size=None,
            norm_before_gate=False,
            is_rms_norm=False,
        )

        success &= allclose_with_info(
            out_native, out_triton, name="Native vs Triton (out)")
        success &= allclose_with_info(
            mean_native, mean_triton, name="Native vs Triton (mean)")
        success &= allclose_with_info(
            rstd_native, rstd_triton, name="Native vs Triton (rstd)")

    return success


def test_case_4_gating_after_norm():
    """æµ‹è¯• 4: Gating AFTER Normalization"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 4: Gating (SwiGLU) AFTER Normalization")
    print("=" * 80)

    # å‡†å¤‡æ•°æ®
    M, N = 16, 128
    x = torch.randn(M, N, dtype=torch.float32, device='cuda')
    weight = torch.randn(N, dtype=torch.float32, device='cuda')
    bias = None  # æµ‹è¯•æ—  bias
    z = torch.randn(M, N, dtype=torch.float32, device='cuda')
    eps = 1e-5

    print(f"è¾“å…¥: x={x.shape}, z={z.shape}, bias=None")

    # Native å®ç°
    out_native, mean_native, rstd_native = _layer_norm_fwd_native(
        x, weight, bias, eps,
        z=z,
        group_size=None,
        norm_before_gate=True,  # Gating AFTER norm
        is_rms_norm=False,
    )

    print(f"Native è¾“å‡º: out={out_native.shape}")

    success = True

    if HAS_TRITON:
        # ä¸ Triton å®ç°å¯¹æ¯”
        out_triton, mean_triton, rstd_triton = _layer_norm_fwd_triton(
            x.clone(), weight.clone(), bias, eps,
            z=z.clone(),
            group_size=None,
            norm_before_gate=True,
            is_rms_norm=False,
        )

        success &= allclose_with_info(
            out_native, out_triton, name="Native vs Triton (out)")
        success &= allclose_with_info(
            mean_native, mean_triton, name="Native vs Triton (mean)")
        success &= allclose_with_info(
            rstd_native, rstd_triton, name="Native vs Triton (rstd)")

    return success


def test_case_5_groupnorm():
    """æµ‹è¯• 5: GroupNorm"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 5: GroupNorm (group_size < N)")
    print("=" * 80)

    # å‡†å¤‡æ•°æ®
    M, N = 32, 768
    group_size = 96  # 8 groups
    x = torch.randn(M, N, dtype=torch.float32, device='cuda')
    weight = torch.randn(N, dtype=torch.float32, device='cuda')
    bias = torch.randn(N, dtype=torch.float32, device='cuda')
    eps = 1e-5

    print(f"è¾“å…¥: x={x.shape}, group_size={group_size} ({N // group_size} groups)")

    # Native å®ç°
    out_native, mean_native, rstd_native = _layer_norm_fwd_native(
        x, weight, bias, eps,
        z=None,
        group_size=group_size,
        norm_before_gate=True,
        is_rms_norm=False,
    )

    print(
        f"Native è¾“å‡º: out={out_native.shape}, mean={mean_native.shape}, rstd={rstd_native.shape}")

    success = True

    if HAS_TRITON:
        # ä¸ Triton å®ç°å¯¹æ¯”
        out_triton, mean_triton, rstd_triton = _layer_norm_fwd_triton(
            x.clone(), weight.clone(), bias.clone(), eps,
            z=None,
            group_size=group_size,
            norm_before_gate=True,
            is_rms_norm=False,
        )

        success &= allclose_with_info(
            out_native, out_triton, name="Native vs Triton (out)")
        success &= allclose_with_info(
            mean_native, mean_triton, name="Native vs Triton (mean)")
        success &= allclose_with_info(
            rstd_native, rstd_triton, name="Native vs Triton (rstd)")

    return success


def test_case_6_complex_combination():
    """æµ‹è¯• 6: å¤æ‚ç»„åˆï¼ˆRMSNorm + GroupNorm + Gatingï¼‰"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 6: å¤æ‚ç»„åˆ (RMSNorm + GroupNorm + Gating)")
    print("=" * 80)

    # å‡†å¤‡æ•°æ®
    M, N = 64, 1024
    group_size = 128  # 8 groups
    x = torch.randn(M, N, dtype=torch.bfloat16, device='cuda')
    weight = torch.randn(N, dtype=torch.bfloat16, device='cuda')
    bias = None
    z = torch.randn(M, N, dtype=torch.bfloat16, device='cuda')
    eps = 1e-6

    print(f"è¾“å…¥: x={x.shape}, group_size={group_size}, z={z.shape}, bias=None")
    print(f"æ¨¡å¼: RMSNorm + GroupNorm + Gating (before norm)")

    # Native å®ç°
    out_native, mean_native, rstd_native = _layer_norm_fwd_native(
        x, weight, bias, eps,
        z=z,
        group_size=group_size,
        norm_before_gate=False,  # Gating before norm
        is_rms_norm=True,  # RMSNorm
    )

    print(f"Native è¾“å‡º: out={out_native.shape}")
    assert mean_native is None, "RMSNorm should not compute mean"

    success = True

    if HAS_TRITON:
        # ä¸ Triton å®ç°å¯¹æ¯”
        out_triton, mean_triton, rstd_triton = _layer_norm_fwd_triton(
            x.clone(), weight.clone(), bias, eps,
            z=z.clone(),
            group_size=group_size,
            norm_before_gate=False,
            is_rms_norm=True,
        )

        success &= allclose_with_info(
            out_native, out_triton, rtol=1e-3, atol=1e-4, name="Native vs Triton (out)")
        success &= allclose_with_info(
            rstd_native, rstd_triton, rtol=1e-3, atol=1e-4, name="Native vs Triton (rstd)")
        assert mean_triton is None

    return success


def test_case_7_high_level_api():
    """æµ‹è¯• 7: é«˜å±‚ APIï¼ˆç”¨æˆ·æ¥å£ï¼‰"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 7: é«˜å±‚ API (layernorm_fn_native, rmsnorm_fn_native)")
    print("=" * 80)

    # å‡†å¤‡æ•°æ®
    batch_size, seq_len, hidden_dim = 4, 128, 768
    x = torch.randn(batch_size, seq_len, hidden_dim,
                    dtype=torch.float32, device='cuda')
    weight = torch.randn(hidden_dim, dtype=torch.float32, device='cuda')
    bias = torch.randn(hidden_dim, dtype=torch.float32, device='cuda')
    z = torch.randn(batch_size, seq_len, hidden_dim,
                    dtype=torch.float32, device='cuda')
    eps = 1e-5

    print(f"è¾“å…¥: x={x.shape} (3D)")

    success = True

    # LayerNorm
    print("\n  æµ‹è¯• layernorm_fn_native:")
    out_native_ln = layernorm_fn_native(x, weight, bias, eps=eps)
    print(f"    è¾“å‡º: {out_native_ln.shape}")

    if HAS_TRITON:
        out_triton_ln = layernorm_fn_triton(
            x.clone(), weight.clone(), bias.clone(), eps=eps)
        success &= allclose_with_info(
            out_native_ln, out_triton_ln, name="    LayerNorm")

    # RMSNorm
    print("\n  æµ‹è¯• rmsnorm_fn_native:")
    out_native_rms = rmsnorm_fn_native(x, weight, bias, eps=eps)
    print(f"    è¾“å‡º: {out_native_rms.shape}")

    if HAS_TRITON:
        out_triton_rms = rmsnorm_fn_triton(
            x.clone(), weight.clone(), bias.clone(), eps=eps)
        success &= allclose_with_info(
            out_native_rms, out_triton_rms, rtol=1e-3, atol=1e-4, name="    RMSNorm")

    # LayerNorm + Gating
    print("\n  æµ‹è¯• layernorm_fn_native with gating:")
    out_native_gate = layernorm_fn_native(
        x, weight, bias, z=z, eps=eps, norm_before_gate=True)
    print(f"    è¾“å‡º: {out_native_gate.shape}")

    if HAS_TRITON:
        out_triton_gate = layernorm_fn_triton(x.clone(), weight.clone(
        ), bias.clone(), z=z.clone(), eps=eps, norm_before_gate=True)
        success &= allclose_with_info(
            out_native_gate, out_triton_gate, name="    LayerNorm + Gating")

    return success


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "â•”" + "=" * 78 + "â•—")
    print("â•‘" + " " * 20 + "LayerNorm Native å®ç°æµ‹è¯•å¥—ä»¶" + " " * 28 + "â•‘")
    print("â•š" + "=" * 78 + "â•")

    if not torch.cuda.is_available():
        print("\nâŒ é”™è¯¯: CUDA ä¸å¯ç”¨ï¼Œæµ‹è¯•éœ€è¦ GPU")
        return False

    print(f"\nâœ… CUDA å¯ç”¨: {torch.cuda.get_device_name()}")
    print(f"âœ… Triton å®ç°: {'å¯ç”¨' if HAS_TRITON else 'ä¸å¯ç”¨ï¼ˆä»…æµ‹è¯• nativeï¼‰'}")

    results = []

    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    tests = [
        ("æ ‡å‡† LayerNorm", test_case_1_standard_layernorm),
        ("RMSNorm", test_case_2_rmsnorm),
        ("Gating Before Norm", test_case_3_gating_before_norm),
        ("Gating After Norm", test_case_4_gating_after_norm),
        ("GroupNorm", test_case_5_groupnorm),
        ("å¤æ‚ç»„åˆ", test_case_6_complex_combination),
        ("é«˜å±‚ API", test_case_7_high_level_api),
    ]

    for test_name, test_func in tests:
        try:
            success = test_func()
            results.append((test_name, success))
        except Exception as e:
            print(f"\n  âŒ æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            results.append((test_name, False))

    # æ€»ç»“
    print("\n" + "=" * 80)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 80)

    for test_name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{test_name:30s}: {status}")

    all_passed = all(success for _, success in results)

    if all_passed:
        print("\n" + "ğŸ‰ " * 20)
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Native å®ç°ä¸ Triton kernel å®Œå…¨ç­‰ä»·")
        print("ğŸ‰ " * 20)
    else:
        print("\n" + "âš ï¸  " * 20)
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°")
        print("âš ï¸  " * 20)

    return all_passed


if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)
