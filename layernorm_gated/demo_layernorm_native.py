#!/usr/bin/env python3
"""
LayerNorm Native å®ç°æ¼”ç¤ºè„šæœ¬

å±•ç¤ºå¦‚ä½•ä½¿ç”¨ PyTorch native å®ç°æ›¿æ¢ Triton kernel
"""

import torch
from layernorm_native_implementation import (
    layernorm_fn_native,
    rmsnorm_fn_native,
    simple_layernorm_native,
)


def demo_1_basic_layernorm():
    """ç¤ºä¾‹ 1: åŸºæœ¬ LayerNorm"""
    print("\n" + "=" * 80)
    print("ç¤ºä¾‹ 1: åŸºæœ¬ LayerNorm")
    print("=" * 80)

    # å‡†å¤‡æ•°æ®
    batch_size, seq_len, hidden_dim = 2, 10, 64
    x = torch.randn(batch_size, seq_len, hidden_dim)
    weight = torch.ones(hidden_dim)
    bias = torch.zeros(hidden_dim)

    print(f"è¾“å…¥: x.shape = {x.shape}")

    # Native å®ç°
    out_native = layernorm_fn_native(x, weight, bias, eps=1e-5)
    print(f"è¾“å‡º: out.shape = {out_native.shape}")
    print(f"      out.mean() = {out_native.mean():.6f}")
    print(f"      out.std() = {out_native.std():.6f}")

    # ä¸ PyTorch æ ‡å‡†å®ç°å¯¹æ¯”
    layer_norm = torch.nn.LayerNorm(hidden_dim)
    layer_norm.weight.data = weight
    layer_norm.bias.data = bias
    out_torch = layer_norm(x)

    # éªŒè¯
    max_diff = (out_native - out_torch).abs().max().item()
    print(f"\nä¸ PyTorch LayerNorm å¯¹æ¯”:")
    print(f"  æœ€å¤§å·®å¼‚: {max_diff:.2e}")
    print(f"  æ˜¯å¦ç­‰ä»·: {'âœ… Yes' if max_diff < 1e-5 else 'âŒ No'}")


def demo_2_rmsnorm():
    """ç¤ºä¾‹ 2: RMSNorm"""
    print("\n" + "=" * 80)
    print("ç¤ºä¾‹ 2: RMSNorm (ä¸è®¡ç®— mean)")
    print("=" * 80)

    # å‡†å¤‡æ•°æ®
    batch_size, seq_len, hidden_dim = 4, 20, 128
    x = torch.randn(batch_size, seq_len, hidden_dim)
    weight = torch.ones(hidden_dim)
    bias = torch.zeros(hidden_dim)

    print(f"è¾“å…¥: x.shape = {x.shape}")

    # RMSNorm
    out_rms = rmsnorm_fn_native(x, weight, bias, eps=1e-6)
    print(f"è¾“å‡º: out.shape = {out_rms.shape}")
    print(f"      out.mean() = {out_rms.mean():.6f}")
    print(f"      out.std() = {out_rms.std():.6f}")

    # æ‰‹åŠ¨éªŒè¯
    rms = torch.sqrt((x ** 2).mean(dim=-1, keepdim=True) + 1e-6)
    out_manual = (x / rms) * weight + bias

    max_diff = (out_rms - out_manual).abs().max().item()
    print(f"\nä¸æ‰‹åŠ¨è®¡ç®—å¯¹æ¯”:")
    print(f"  æœ€å¤§å·®å¼‚: {max_diff:.2e}")
    print(f"  æ˜¯å¦ç­‰ä»·: {'âœ… Yes' if max_diff < 1e-5 else 'âŒ No'}")


def demo_3_gating():
    """ç¤ºä¾‹ 3: SwiGLU Gating"""
    print("\n" + "=" * 80)
    print("ç¤ºä¾‹ 3: SwiGLU Gating (é—¨æ§æ¿€æ´»)")
    print("=" * 80)

    # å‡†å¤‡æ•°æ®
    batch_size, seq_len, hidden_dim = 2, 10, 64
    x = torch.randn(batch_size, seq_len, hidden_dim)
    z = torch.randn(batch_size, seq_len, hidden_dim)  # é—¨æ§å€¼
    weight = torch.ones(hidden_dim)
    bias = torch.zeros(hidden_dim)

    print(f"è¾“å…¥: x.shape = {x.shape}")
    print(f"      z.shape = {z.shape} (gating)")

    # Gating BEFORE norm
    out_before = layernorm_fn_native(
        x, weight, bias,
        z=z,
        eps=1e-5,
        norm_before_gate=False  # å…ˆ gate å norm
    )
    print(f"\nGating BEFORE norm:")
    print(f"  è¾“å‡º: out.shape = {out_before.shape}")
    print(f"        out.mean() = {out_before.mean():.6f}")

    # Gating AFTER norm
    out_after = layernorm_fn_native(
        x, weight, bias,
        z=z,
        eps=1e-5,
        norm_before_gate=True  # å…ˆ norm å gate
    )
    print(f"\nGating AFTER norm:")
    print(f"  è¾“å‡º: out.shape = {out_after.shape}")
    print(f"        out.mean() = {out_after.mean():.6f}")

    print(f"\nä¸¤ç§æ–¹å¼çš„å·®å¼‚:")
    diff = (out_before - out_after).abs().mean().item()
    print(f"  å¹³å‡å·®å¼‚: {diff:.6f}")
    print(f"  è¯´æ˜: é—¨æ§é¡ºåºå½±å“ç»“æœï¼ˆè¿™æ˜¯æ­£å¸¸çš„ï¼‰")


def demo_4_groupnorm():
    """ç¤ºä¾‹ 4: GroupNorm"""
    print("\n" + "=" * 80)
    print("ç¤ºä¾‹ 4: GroupNorm (åˆ†ç»„å½’ä¸€åŒ–)")
    print("=" * 80)

    # å‡†å¤‡æ•°æ®
    batch_size, seq_len, hidden_dim = 2, 10, 96
    group_size = 32  # 3 groups
    x = torch.randn(batch_size, seq_len, hidden_dim)
    weight = torch.ones(hidden_dim)
    bias = torch.zeros(hidden_dim)

    print(f"è¾“å…¥: x.shape = {x.shape}")
    print(f"      hidden_dim = {hidden_dim}")
    print(f"      group_size = {group_size}")
    print(f"      num_groups = {hidden_dim // group_size}")

    # GroupNorm
    out_group = layernorm_fn_native(
        x, weight, bias,
        eps=1e-5,
        group_size=group_size
    )
    print(f"\nè¾“å‡º: out.shape = {out_group.shape}")
    print(f"      out.mean() = {out_group.mean():.6f}")
    print(f"      out.std() = {out_group.std():.6f}")

    # ä¸æ ‡å‡† LayerNorm å¯¹æ¯”ï¼ˆgroup_size=hidden_dimï¼‰
    out_standard = layernorm_fn_native(
        x, weight, bias,
        eps=1e-5,
        group_size=None  # ç­‰ä»·äº hidden_dim
    )

    diff = (out_group - out_standard).abs().mean().item()
    print(f"\nä¸æ ‡å‡† LayerNorm å¯¹æ¯”:")
    print(f"  å¹³å‡å·®å¼‚: {diff:.6f}")
    print(f"  è¯´æ˜: GroupNorm ç»“æœä¸åŒï¼ˆè¿™æ˜¯æ­£å¸¸çš„ï¼‰")


def demo_5_replace_example():
    """ç¤ºä¾‹ 5: å¦‚ä½•æ›¿æ¢ç°æœ‰ä»£ç """
    print("\n" + "=" * 80)
    print("ç¤ºä¾‹ 5: æ›¿æ¢ç°æœ‰ä»£ç ")
    print("=" * 80)

    # å‡†å¤‡æ•°æ®
    x = torch.randn(4, 128, 768)
    weight = torch.ones(768)
    bias = torch.zeros(768)

    print("åŸä»£ç :")
    print("  from python.sglang.srt.layers.attention.fla.layernorm_gated import layernorm_fn")
    print("  out = layernorm_fn(x, weight, bias, eps=1e-5)")

    print("\næ›¿æ¢ä¸º:")
    print("  from layernorm_native_implementation import layernorm_fn_native as layernorm_fn")
    print("  out = layernorm_fn(x, weight, bias, eps=1e-5)")

    # å®é™…æ‰§è¡Œ
    from layernorm_native_implementation import layernorm_fn_native as layernorm_fn
    out = layernorm_fn(x, weight, bias, eps=1e-5)

    print(f"\næ‰§è¡Œç»“æœ:")
    print(f"  è¾“å…¥: x.shape = {x.shape}")
    print(f"  è¾“å‡º: out.shape = {out.shape}")
    print(f"  âœ… æ›¿æ¢æˆåŠŸï¼")


def demo_6_simple_version():
    """ç¤ºä¾‹ 6: ç®€åŒ–ç‰ˆ LayerNorm"""
    print("\n" + "=" * 80)
    print("ç¤ºä¾‹ 6: ç®€åŒ–ç‰ˆ LayerNorm (ä»…æ ‡å‡†åŠŸèƒ½)")
    print("=" * 80)

    # å‡†å¤‡æ•°æ®
    x = torch.randn(32, 256)
    weight = torch.ones(256)
    bias = torch.zeros(256)

    print(f"è¾“å…¥: x.shape = {x.shape}")

    # ç®€åŒ–ç‰ˆ
    out_simple = simple_layernorm_native(x, weight, bias, eps=1e-5)
    print(f"è¾“å‡º: out.shape = {out_simple.shape}")

    # ä¸ PyTorch å¯¹æ¯”
    out_torch = torch.nn.functional.layer_norm(
        x, (256,), weight, bias, eps=1e-5)

    max_diff = (out_simple - out_torch).abs().max().item()
    print(f"\nä¸ PyTorch å¯¹æ¯”:")
    print(f"  æœ€å¤§å·®å¼‚: {max_diff:.2e}")
    print(f"  æ˜¯å¦ç­‰ä»·: {'âœ… Yes' if max_diff < 1e-5 else 'âŒ No'}")


def main():
    """è¿è¡Œæ‰€æœ‰æ¼”ç¤º"""
    print("\n" + "â•”" + "=" * 78 + "â•—")
    print("â•‘" + " " * 22 + "LayerNorm Native æ¼”ç¤ºè„šæœ¬" + " " * 32 + "â•‘")
    print("â•š" + "=" * 78 + "â•")

    print(f"\nâœ… PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"âœ… è®¾å¤‡: {'CUDA' if torch.cuda.is_available() else 'CPU'}")

    # è¿è¡Œæ‰€æœ‰æ¼”ç¤º
    demos = [
        demo_1_basic_layernorm,
        demo_2_rmsnorm,
        demo_3_gating,
        demo_4_groupnorm,
        demo_5_replace_example,
        demo_6_simple_version,
    ]

    for demo in demos:
        try:
            demo()
        except Exception as e:
            print(f"\nâŒ æ¼”ç¤ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

    # æ€»ç»“
    print("\n" + "=" * 80)
    print("æ¼”ç¤ºæ€»ç»“")
    print("=" * 80)
    print("âœ… åŸºæœ¬ LayerNorm - ä¸ PyTorch å®Œå…¨ç­‰ä»·")
    print("âœ… RMSNorm - ä¸è®¡ç®— meanï¼Œæ›´é«˜æ•ˆ")
    print("âœ… SwiGLU Gating - æ”¯æŒé—¨æ§å‰/å")
    print("âœ… GroupNorm - æ”¯æŒåˆ†ç»„å½’ä¸€åŒ–")
    print("âœ… ä»£ç æ›¿æ¢ - å³æ’å³ç”¨ï¼Œæ— éœ€ä¿®æ”¹å…¶ä»–ä»£ç ")
    print("âœ… ç®€åŒ–ç‰ˆ - ä»…æ ‡å‡†åŠŸèƒ½ï¼Œä»£ç æ›´ç®€æ´")

    print("\n" + "ğŸ‰ " * 20)
    print("âœ… æ¼”ç¤ºå®Œæˆï¼Native å®ç°å·²å°±ç»ªï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨")
    print("ğŸ‰ " * 20)

    print("\nğŸ“š æ›´å¤šä¿¡æ¯è¯·æŸ¥çœ‹:")
    print("  - layernorm_native_implementation.py  (æ ¸å¿ƒå®ç°)")
    print("  - LAYERNORM_NATIVE_USAGE_GUIDE.md     (è¯¦ç»†ä½¿ç”¨æŒ‡å—)")
    print("  - LAYERNORM_MIGRATION_SUMMARY.md      (é¡¹ç›®æ€»ç»“)")
    print("  - test_layernorm_native_cpu.py        (æµ‹è¯•å¥—ä»¶)")


if __name__ == "__main__":
    main()
