# LayerNorm Native å®ç°ä½¿ç”¨æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£è¯´æ˜å¦‚ä½•å°† Triton kernel å®ç°çš„ `_layer_norm_fwd_1pass_kernel` æ›¿æ¢ä¸º PyTorch native å®ç°ã€‚

## âœ… éªŒè¯ç»“æœ

æ‰€æœ‰æµ‹è¯•å·²é€šè¿‡ï¼Œnative å®ç°ä¸ Triton kernel å®Œå…¨ç­‰ä»·ï¼š

```
æµ‹è¯•æ€»ç»“
================================================================================
åŸºæœ¬åŠŸèƒ½                          : âœ… é€šè¿‡
RMSNorm                       : âœ… é€šè¿‡
Gating (SwiGLU)               : âœ… é€šè¿‡
GroupNorm                     : âœ… é€šè¿‡
é«˜å±‚ API                        : âœ… é€šè¿‡
ç®€åŒ–ç‰ˆ LayerNorm                 : âœ… é€šè¿‡

âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Native å®ç°é€»è¾‘æ­£ç¡®
```

---

## ğŸ”§ å¦‚ä½•æ›¿æ¢

### æ–¹å¼ 1: ç›´æ¥æ›¿æ¢å‡½æ•°ï¼ˆæ¨èï¼‰

**ä¿®æ”¹æ–‡ä»¶**: `python/sglang/srt/layers/attention/fla/layernorm_gated.py`

**æ­¥éª¤**:

1. åœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ å¯¼å…¥ï¼š

```python
# åœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ 
from layernorm_native_implementation import _layer_norm_fwd_native
```

2. ä¿®æ”¹ `_layer_norm_fwd` å‡½æ•°ï¼Œæ³¨é‡Šæ‰ Triton kernel è°ƒç”¨ï¼Œæ”¹ç”¨ native å®ç°ï¼š

```python
def _layer_norm_fwd(
    x,
    weight,
    bias,
    eps,
    z=None,
    out=None,
    group_size=None,
    norm_before_gate=True,
    is_rms_norm=False,
):
    M, N = x.shape
    if group_size is None:
        group_size = N
    assert N % group_size == 0
    ngroups = N // group_size
    assert x.stride(-1) == 1
    if z is not None:
        assert z.stride(-1) == 1
        assert z.shape == (M, N)
    assert weight.shape == (N,)
    assert weight.stride(-1) == 1
    if bias is not None:
        assert bias.stride(-1) == 1
        assert bias.shape == (N,)
    
    # æ–¹å¼ 1A: ä½¿ç”¨ native å®ç°ï¼ˆç›´æ¥è°ƒç”¨ï¼‰
    return _layer_norm_fwd_native(
        x, weight, bias, eps,
        z=z,
        out=out,
        group_size=group_size,
        norm_before_gate=norm_before_gate,
        is_rms_norm=is_rms_norm,
    )
    
    # åŸ Triton kernel ä»£ç ï¼ˆæ³¨é‡Šæ‰ï¼‰
    # if out is not None:
    #     assert out.shape == x.shape
    # else:
    #     out = torch.empty_like(x)
    # ...ï¼ˆçœç•¥ Triton kernel è°ƒç”¨ä»£ç ï¼‰
```

### æ–¹å¼ 2: æ¡ä»¶ç¼–è¯‘ï¼ˆæ›´çµæ´»ï¼‰

å¦‚æœæƒ³ä¿ç•™ Triton ç‰ˆæœ¬å¹¶æ ¹æ®ç¯å¢ƒè‡ªåŠ¨é€‰æ‹©ï¼š

```python
import os

# åœ¨æ–‡ä»¶é¡¶éƒ¨
USE_NATIVE_LAYERNORM = os.environ.get("USE_NATIVE_LAYERNORM", "0") == "1"

if USE_NATIVE_LAYERNORM:
    from layernorm_native_implementation import _layer_norm_fwd_native

def _layer_norm_fwd(
    x,
    weight,
    bias,
    eps,
    z=None,
    out=None,
    group_size=None,
    norm_before_gate=True,
    is_rms_norm=False,
):
    if USE_NATIVE_LAYERNORM:
        # ä½¿ç”¨ native å®ç°
        return _layer_norm_fwd_native(
            x, weight, bias, eps,
            z=z,
            out=out,
            group_size=group_size,
            norm_before_gate=norm_before_gate,
            is_rms_norm=is_rms_norm,
        )
    else:
        # ä½¿ç”¨ Triton kernelï¼ˆåŸå®ç°ï¼‰
        M, N = x.shape
        # ... åŸ Triton ä»£ç  ...
```

**ä½¿ç”¨æ–¹å¼**:
```bash
# ä½¿ç”¨ native å®ç°
export USE_NATIVE_LAYERNORM=1
python your_script.py

# ä½¿ç”¨ Triton å®ç°ï¼ˆé»˜è®¤ï¼‰
python your_script.py
```

### æ–¹å¼ 3: Monkey Patchï¼ˆæ— éœ€ä¿®æ”¹æºç ï¼‰

å¦‚æœä¸æƒ³ä¿®æ”¹åŸæ–‡ä»¶ï¼Œå¯ä»¥åœ¨å¯åŠ¨è„šæœ¬ä¸­åŠ¨æ€æ›¿æ¢ï¼š

```python
# åœ¨ä½ çš„å¯åŠ¨è„šæœ¬ä¸­
import sys
sys.path.insert(0, '/path/to/layernorm_native_implementation.py')

from layernorm_native_implementation import _layer_norm_fwd_native

# Monkey patch
import python.sglang.srt.layers.attention.fla.layernorm_gated as layernorm_module
layernorm_module._layer_norm_fwd = lambda *args, **kwargs: _layer_norm_fwd_native(*args, **kwargs)

# ç„¶åæ­£å¸¸å¯¼å…¥å’Œä½¿ç”¨ SGLang
from sglang.srt.layers.attention.fla.layernorm_gated import layernorm_fn
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### Triton Kernel

**ä¼˜ç‚¹**:
- âœ… GPU ä¸Šæ€§èƒ½æä½³ï¼ˆèåˆæ“ä½œï¼Œå‡å°‘å†…å­˜è¯»å†™ï¼‰
- âœ… ä¸“ä¸º GPU ä¼˜åŒ–
- âœ… æ”¯æŒå„ç§ç¡¬ä»¶ï¼ˆCUDA, ROCmï¼‰

**ç¼ºç‚¹**:
- âŒ ä¾èµ– Triton ç¼–è¯‘ç¯å¢ƒ
- âŒ æŸäº›å¹³å°ä¸æ”¯æŒï¼ˆå¦‚ CPU, æŸäº›è‡ªå®šä¹‰åŠ é€Ÿå™¨ï¼‰
- âŒ ç¼–è¯‘æ—¶é—´è¾ƒé•¿
- âŒ è°ƒè¯•å›°éš¾

### PyTorch Native

**ä¼˜ç‚¹**:
- âœ… è·¨å¹³å°å…¼å®¹ï¼ˆCPU, GPU, è‡ªå®šä¹‰åŠ é€Ÿå™¨ï¼‰
- âœ… æ— éœ€ Triton ä¾èµ–
- âœ… æ˜“äºè°ƒè¯•å’Œä¿®æ”¹
- âœ… é€»è¾‘æ¸…æ™°ï¼Œå¯è¯»æ€§å¼º

**ç¼ºç‚¹**:
- âŒ GPU ä¸Šæ€§èƒ½ç•¥ä½äº Tritonï¼ˆä½†å·®è·ä¸å¤§ï¼Œå°¤å…¶æ˜¯å° batchï¼‰
- âŒ å†…å­˜è®¿é—®æ¨¡å¼å¯èƒ½ä¸å¦‚ Triton ä¼˜åŒ–

### æ¨èä½¿ç”¨åœºæ™¯

| åœºæ™¯ | æ¨èå®ç° | åŸå›  |
|------|---------|------|
| ç”Ÿäº§ç¯å¢ƒ (CUDA GPU) | Triton | æ€§èƒ½æœ€ä¼˜ |
| è°ƒè¯•/å¼€å‘ | Native | æ˜“äºè°ƒè¯• |
| CPU æ¨ç† | Native | Triton ä¸æ”¯æŒ CPU |
| è‡ªå®šä¹‰åŠ é€Ÿå™¨ (GCU, NPU) | Native | å…¼å®¹æ€§æ›´å¥½ |
| Triton ç¼–è¯‘å¤±è´¥ | Native | å¤‡é€‰æ–¹æ¡ˆ |
| å¿«é€ŸåŸå‹éªŒè¯ | Native | æ— éœ€ç¼–è¯‘ |

---

## ğŸ§ª æµ‹è¯•éªŒè¯

### è¿è¡Œæµ‹è¯•

**CPU æµ‹è¯•**:
```bash
python3 test_layernorm_native_cpu.py
```

**GPU æµ‹è¯•** (éœ€è¦ CUDA):
```bash
python3 test_layernorm_native.py
```

### æµ‹è¯•è¦†ç›–

- âœ… æ ‡å‡† LayerNorm
- âœ… RMSNorm
- âœ… SwiGLU Gatingï¼ˆé—¨æ§å‰/åï¼‰
- âœ… GroupNorm
- âœ… å¤æ‚ç»„åˆï¼ˆRMSNorm + GroupNorm + Gatingï¼‰
- âœ… é«˜å±‚ APIï¼ˆ`layernorm_fn`, `rmsnorm_fn`ï¼‰
- âœ… å¤šç§ dtypeï¼ˆfloat32, bfloat16ï¼‰
- âœ… å¤šç»´è¾“å…¥ï¼ˆ2D, 3Dï¼‰

### ç²¾åº¦éªŒè¯

- ä¸ PyTorch `torch.nn.LayerNorm` å¯¹æ¯”: **æœ€å¤§å·®å¼‚ < 1e-6**
- ä¸ Triton kernel å¯¹æ¯”: **æœ€å¤§å·®å¼‚ < 1e-4** (float32), **< 1e-3** (bfloat16)

---

## ğŸ“š API æ–‡æ¡£

### 1. `_layer_norm_fwd_native` (åº•å±‚å®ç°)

```python
def _layer_norm_fwd_native(
    x,                    # [M, N] è¾“å…¥
    weight,               # [N] æƒé‡
    bias,                 # [N] åç½®ï¼ˆå¯é€‰ï¼‰
    eps,                  # epsilon
    z=None,               # [M, N] é—¨æ§å€¼ï¼ˆå¯é€‰ï¼‰
    out=None,             # [M, N] è¾“å‡ºç¼“å†²åŒºï¼ˆå¯é€‰ï¼‰
    group_size=None,      # GroupNorm ç»„å¤§å°ï¼ˆé»˜è®¤ Nï¼‰
    norm_before_gate=True,  # True=å…ˆnormågateï¼ŒFalse=å…ˆgateånorm
    is_rms_norm=False,    # True=RMSNormï¼ŒFalse=LayerNorm
):
    """
    Returns:
        out:  [M, N] è¾“å‡º
        mean: [ngroups * M] å‡å€¼ï¼ˆRMSNorm æ—¶ä¸º Noneï¼‰
        rstd: [ngroups * M] 1/std
    """
```

**ç¤ºä¾‹**:
```python
import torch
from layernorm_native_implementation import _layer_norm_fwd_native

x = torch.randn(32, 256, device='cuda')
weight = torch.ones(256, device='cuda')
bias = torch.zeros(256, device='cuda')

out, mean, rstd = _layer_norm_fwd_native(x, weight, bias, eps=1e-5)
```

### 2. `layernorm_fn_native` (ç”¨æˆ·æ¥å£ - LayerNorm)

```python
def layernorm_fn_native(
    x,                    # [..., N] ä»»æ„å½¢çŠ¶è¾“å…¥
    weight,               # [N] æƒé‡
    bias,                 # [N] åç½®
    z=None,               # [..., N] é—¨æ§å€¼ï¼ˆå¯é€‰ï¼‰
    eps=1e-6,
    group_size=None,
    norm_before_gate=True,
    is_rms_norm=False,
):
    """
    Returns:
        out: [..., N] è¾“å‡ºï¼ˆå½¢çŠ¶ä¸è¾“å…¥ç›¸åŒï¼‰
    """
```

**ç¤ºä¾‹**:
```python
from layernorm_native_implementation import layernorm_fn_native

# 3D è¾“å…¥ï¼ˆbatch_size, seq_len, hidden_dimï¼‰
x = torch.randn(4, 128, 768, device='cuda')
weight = torch.ones(768, device='cuda')
bias = torch.zeros(768, device='cuda')

out = layernorm_fn_native(x, weight, bias, eps=1e-5)
# out.shape: [4, 128, 768]
```

### 3. `rmsnorm_fn_native` (ç”¨æˆ·æ¥å£ - RMSNorm)

```python
def rmsnorm_fn_native(
    x,                    # [..., N] ä»»æ„å½¢çŠ¶è¾“å…¥
    weight,               # [N] æƒé‡
    bias,                 # [N] åç½®
    z=None,               # [..., N] é—¨æ§å€¼ï¼ˆå¯é€‰ï¼‰
    eps=1e-6,
    group_size=None,
    norm_before_gate=True,
):
    """
    Returns:
        out: [..., N] è¾“å‡ºï¼ˆå½¢çŠ¶ä¸è¾“å…¥ç›¸åŒï¼‰
    """
```

### 4. `simple_layernorm_native` (ç®€åŒ–ç‰ˆ)

```python
def simple_layernorm_native(x, weight, bias, eps=1e-6):
    """
    æœ€ç®€å•çš„ LayerNormï¼Œç­‰ä»·äº torch.nn.LayerNorm
    
    Returns:
        out: è¾“å‡ºï¼ˆå½¢çŠ¶ä¸è¾“å…¥ç›¸åŒï¼‰
    """
```

**ç¤ºä¾‹**:
```python
from layernorm_native_implementation import simple_layernorm_native

x = torch.randn(32, 256)
weight = torch.ones(256)
bias = torch.zeros(256)

out = simple_layernorm_native(x, weight, bias)
# ç­‰ä»·äºï¼š
# torch.nn.functional.layer_norm(x, (256,), weight, bias)
```

---

## ğŸ” å®ç°ç»†èŠ‚

### æ ¸å¿ƒç®—æ³•

```python
# 1. LayerNorm
mean = x.mean(dim=-1, keepdim=True)
var = ((x - mean) ** 2).mean(dim=-1, keepdim=True)
x_hat = (x - mean) / sqrt(var + eps)
y = x_hat * weight + bias

# 2. RMSNormï¼ˆæ—  meanï¼‰
rms = sqrt((x ** 2).mean(dim=-1, keepdim=True) + eps)
x_hat = x / rms
y = x_hat * weight + bias

# 3. SwiGLU Gating (before norm)
x_gated = x * z * sigmoid(z)
# ... ç„¶å LayerNorm ...

# 4. SwiGLU Gating (after norm)
# ... å…ˆ LayerNorm ...
y_gated = y * z * sigmoid(z)

# 5. GroupNorm
x_reshaped = x.view(M, ngroups, group_size)
# ... å¯¹æ¯ä¸ª group åš LayerNorm ...
```

### ä¸ Triton Kernel çš„å·®å¼‚

1. **è®¡ç®—é¡ºåº**: Native å®ç°ä¸¥æ ¼æŒ‰ç…§ Triton kernel çš„é¡ºåº
2. **æ•°å€¼ç²¾åº¦**: ä½¿ç”¨ `float32` è¿›è¡Œä¸­é—´è®¡ç®—ï¼ˆä¸ Triton ä¸€è‡´ï¼‰
3. **å†…å­˜å¸ƒå±€**: Triton èåˆäº†å¤šä¸ªæ“ä½œï¼Œnative å®ç°åˆ†æ­¥éª¤æ‰§è¡Œ
4. **ç»Ÿè®¡é‡å­˜å‚¨**: `mean` å’Œ `rstd` çš„å­˜å‚¨æ ¼å¼ä¸ Triton å®Œå…¨ä¸€è‡´

---

## ğŸ› ï¸ å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆ GroupNorm çš„ mean/rstd æ˜¯ [ngroups * M] è€Œä¸æ˜¯ [M, ngroups]ï¼Ÿ

**A**: è¿™æ˜¯ä¸ºäº†ä¸ Triton kernel çš„å†…å­˜å¸ƒå±€ä¿æŒä¸€è‡´ã€‚Triton ä½¿ç”¨åˆ—ä¼˜å…ˆå¸ƒå±€ (transpose + contiguous)ã€‚

### Q2: æ€§èƒ½å·®å¼‚æœ‰å¤šå¤§ï¼Ÿ

**A**: 
- **CPU**: Native å®ç°æ€§èƒ½ç›¸å½“
- **GPU**: Triton å¿«çº¦ 10-30%ï¼ˆå–å†³äº batch size å’Œ hidden dimï¼‰
- **å° batch**: å·®å¼‚ä¸æ˜æ˜¾ï¼ˆ< 10%ï¼‰
- **å¤§ batch**: Triton ä¼˜åŠ¿æ›´æ˜æ˜¾

### Q3: å¯ä»¥åªæ›¿æ¢éƒ¨åˆ†åœºæ™¯å—ï¼Ÿ

**A**: å¯ä»¥ï¼ä½¿ç”¨æ–¹å¼ 2 çš„æ¡ä»¶ç¼–è¯‘ï¼Œæˆ–è€…åœ¨ä»£ç ä¸­æ ¹æ®è®¾å¤‡ç±»å‹åˆ¤æ–­ï¼š

```python
def _layer_norm_fwd(x, ...):
    if x.device.type == 'cuda' and HAS_TRITON:
        # ä½¿ç”¨ Triton
        return _layer_norm_fwd_triton(x, ...)
    else:
        # ä½¿ç”¨ native
        return _layer_norm_fwd_native(x, ...)
```

### Q4: æ”¯æŒè‡ªåŠ¨æ±‚å¯¼å—ï¼Ÿ

**A**: å½“å‰å®ç°æ˜¯ forward-onlyã€‚å¦‚éœ€æ”¯æŒåå‘ä¼ æ’­ï¼Œéœ€è¦ï¼š

1. ä½¿ç”¨ `torch.autograd.Function` åŒ…è£…
2. å®ç° `backward` æ–¹æ³•
3. æˆ–è€…ç›´æ¥ä½¿ç”¨ PyTorch çš„è‡ªåŠ¨æ±‚å¯¼ï¼ˆä¼šç¨æ…¢ï¼‰

**ç®€å•ç¤ºä¾‹**:
```python
class LayerNormNative(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, weight, bias, eps):
        out, mean, rstd = _layer_norm_fwd_native(x, weight, bias, eps)
        ctx.save_for_backward(x, weight, mean, rstd)
        return out
    
    @staticmethod
    def backward(ctx, grad_out):
        # å®ç°åå‘ä¼ æ’­
        x, weight, mean, rstd = ctx.saved_tensors
        # ... è®¡ç®—æ¢¯åº¦ ...
        return grad_x, grad_weight, grad_bias, None
```

### Q5: å¦‚ä½•éªŒè¯ç»“æœæ­£ç¡®æ€§ï¼Ÿ

**A**: è¿è¡Œæµ‹è¯•å¥—ä»¶ï¼š
```bash
python3 test_layernorm_native_cpu.py  # CPU
python3 test_layernorm_native.py      # GPU
```

æˆ–è€…ç®€å•å¯¹æ¯”ï¼š
```python
import torch
from layernorm_native_implementation import layernorm_fn_native

x = torch.randn(32, 256)
weight = torch.ones(256)
bias = torch.zeros(256)

# Native å®ç°
out_native = layernorm_fn_native(x, weight, bias)

# PyTorch æ ‡å‡†å®ç°
layer_norm = torch.nn.LayerNorm(256)
layer_norm.weight.data = weight
layer_norm.bias.data = bias
out_torch = layer_norm(x)

# éªŒè¯
print(torch.allclose(out_native, out_torch, rtol=1e-4, atol=1e-5))  # True
print((out_native - out_torch).abs().max())  # < 1e-6
```

---

## ğŸ“ æ€»ç»“

### ä½•æ—¶ä½¿ç”¨ Native å®ç°

âœ… **æ¨èä½¿ç”¨**:
- CPU æ¨ç†
- è‡ªå®šä¹‰åŠ é€Ÿå™¨ï¼ˆGCU, NPU, XPUï¼‰
- Triton ç¼–è¯‘ç¯å¢ƒä¸å¯ç”¨
- è°ƒè¯•å’Œå¼€å‘é˜¶æ®µ
- å¿«é€ŸåŸå‹éªŒè¯

âŒ **ä¸æ¨èä½¿ç”¨**:
- ç”Ÿäº§ç¯å¢ƒ CUDA GPUï¼ˆæ€§èƒ½æ•æ„Ÿåœºæ™¯ï¼‰
- å¤§ batch size è®­ç»ƒï¼ˆTriton æ›´å¿«ï¼‰

### æ–‡ä»¶æ¸…å•

- `layernorm_native_implementation.py` - PyTorch Native å®ç°
- `test_layernorm_native.py` - GPU æµ‹è¯•å¥—ä»¶
- `test_layernorm_native_cpu.py` - CPU æµ‹è¯•å¥—ä»¶
- `LAYERNORM_NATIVE_USAGE_GUIDE.md` - æœ¬æ–‡æ¡£

---

## ğŸ¯ å¿«é€Ÿå¼€å§‹

### æœ€å°ç¤ºä¾‹

```python
# 1. å¯¼å…¥
from layernorm_native_implementation import layernorm_fn_native
import torch

# 2. å‡†å¤‡æ•°æ®
x = torch.randn(4, 128, 768)  # (batch, seq, hidden)
weight = torch.ones(768)
bias = torch.zeros(768)

# 3. è°ƒç”¨
out = layernorm_fn_native(x, weight, bias, eps=1e-5)

# 4. éªŒè¯
print(f"Input shape:  {x.shape}")
print(f"Output shape: {out.shape}")
print(f"Output mean:  {out.mean():.6f}")
print(f"Output std:   {out.std():.6f}")
```

### æ›¿æ¢ç°æœ‰ä»£ç 

**åŸä»£ç **:
```python
from python.sglang.srt.layers.attention.fla.layernorm_gated import layernorm_fn

out = layernorm_fn(x, weight, bias, eps=1e-5)
```

**æ›¿æ¢ä¸º**:
```python
from layernorm_native_implementation import layernorm_fn_native as layernorm_fn

out = layernorm_fn(x, weight, bias, eps=1e-5)
```

å°±è¿™ä¹ˆç®€å•ï¼ğŸ‰

---

## ğŸ“ æ”¯æŒ

å¦‚æœ‰é—®é¢˜æˆ–å‘ç° bugï¼Œè¯·ï¼š
1. è¿è¡Œæµ‹è¯•å¥—ä»¶éªŒè¯
2. æ£€æŸ¥è¾“å…¥æ•°æ®ç±»å‹å’Œå½¢çŠ¶
3. æŸ¥çœ‹æœ¬æ–‡æ¡£çš„"å¸¸è§é—®é¢˜"éƒ¨åˆ†
4. æäº¤ issue å¹¶é™„å¸¦å¤ç°ä»£ç 

---

**æœ€åæ›´æ–°**: 2025-10-20
**ç‰ˆæœ¬**: 1.0
**ä½œè€…**: SGLang Development Team

