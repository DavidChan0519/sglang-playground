# `_layer_norm_fwd_1pass_kernel` è¾“å‡ºå½¢çŠ¶åˆ†æ

## æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†åˆ†æ `_layer_norm_fwd_1pass_kernel` Triton kernel çš„å®ç°ï¼Œè¿½è¸ªè¾“å…¥åˆ°è¾“å‡ºçš„å½¢çŠ¶å˜åŒ–è¿‡ç¨‹ã€‚

---

## å‡½æ•°è°ƒç”¨é“¾

### å®Œæ•´è°ƒç”¨é“¾

```
ç”¨æˆ·è°ƒç”¨ LayerNorm.forward(x, z)  æˆ–  layernorm_fn(x, ...)
    â†“
LayerNormFn.apply(x, weight, bias, z, ...)
    â†“
LayerNormFn.forward(ctx, x, weight, bias, z, ...)
    â†“
_layer_norm_fwd(x, weight, bias, eps, z, ...)
    â†“
_layer_norm_fwd_1pass_kernel[grid](X, Y, W, B, Z, ...)
```

---

## å…³é”®ä»£ç åˆ†æ

### ç¬¬ 1 æ­¥ï¼š`LayerNormFn.forward` - è¾“å…¥é¢„å¤„ç†ï¼ˆç¬¬ 200-209 è¡Œï¼‰

```python
x_shape_og = x.shape              # ä¿å­˜åŸå§‹å½¢çŠ¶
x = x.reshape(-1, x.shape[-1])    # é‡å¡‘ä¸º 2D: [*, N] -> [M, N]

if z is not None:
    z = z.reshape(-1, z.shape[-1])  # z ä¹Ÿé‡å¡‘ä¸º [M, N]
```

**åˆ†æ**ï¼š
- `x_shape_og`ï¼šåŸå§‹è¾“å…¥å½¢çŠ¶ï¼Œå¯èƒ½æ˜¯ä»»æ„ç»´åº¦ `[d1, d2, ..., dk, N]`
- `x`ï¼šé‡å¡‘åä¸º 2D å¼ é‡ `[M, N]`ï¼Œå…¶ä¸­ `M = d1 * d2 * ... * dk`
- `N`ï¼šæœ€åä¸€ä¸ªç»´åº¦ï¼ˆç‰¹å¾ç»´åº¦ï¼‰

**ç¤ºä¾‹**ï¼š
```python
# å¦‚æœè¾“å…¥ x.shape = [batch_size, seq_len, hidden_dim]
# é‚£ä¹ˆé‡å¡‘å:
M = batch_size * seq_len
N = hidden_dim
x.shape = [M, N]
```

### ç¬¬ 2 æ­¥ï¼š`_layer_norm_fwd` - åˆ›å»ºè¾“å‡ºå¼ é‡ï¼ˆç¬¬ 127-145 è¡Œï¼‰

```python
M, N = x.shape                    # ç¬¬ 127 è¡Œï¼šæå–å½¢çŠ¶
if group_size is None:
    group_size = N                # ç¬¬ 129 è¡Œï¼šé»˜è®¤ group_size = N
ngroups = N // group_size         # ç¬¬ 131 è¡Œï¼šè®¡ç®—ç»„æ•°

# ç¬¬ 142-145 è¡Œï¼šåˆ›å»ºè¾“å‡ºå¼ é‡
if out is not None:
    assert out.shape == x.shape   # æ–­è¨€ï¼šè¾“å‡ºå½¢çŠ¶å¿…é¡»ç­‰äºè¾“å…¥å½¢çŠ¶
else:
    out = torch.empty_like(x)     # åˆ›å»ºä¸ x å½¢çŠ¶ç›¸åŒçš„è¾“å‡º
```

**å…³é”®ç‚¹**ï¼š
- âœ… **è¾“å‡º `out` çš„å½¢çŠ¶ = è¾“å…¥ `x` çš„å½¢çŠ¶ = `[M, N]`**
- `ngroups`ï¼šç‰¹å¾ç»´åº¦è¢«åˆ†æˆå¤šå°‘ç»„ï¼ˆç”¨äº GroupNormï¼‰
- å¦‚æœ `group_size = N`ï¼ˆé»˜è®¤ï¼‰ï¼Œåˆ™ `ngroups = 1`ï¼ˆæ ‡å‡† LayerNormï¼‰

### ç¬¬ 3 æ­¥ï¼šTriton Kernel é…ç½®ï¼ˆç¬¬ 160 è¡Œï¼‰

```python
grid = (M, ngroups)
```

**è¯´æ˜**ï¼š
- `grid` çš„ç¬¬ 1 ç»´ï¼š`M` - æœ‰å¤šå°‘è¡Œéœ€è¦å¤„ç†
- `grid` çš„ç¬¬ 2 ç»´ï¼š`ngroups` - æ¯è¡Œæœ‰å¤šå°‘ç»„
- æ¯ä¸ª Triton ç¨‹åºå¤„ç†è¾“å…¥çš„ä¸€ä¸ª `(row, group)` å—

### ç¬¬ 4 æ­¥ï¼šTriton Kernel å®ç°ï¼ˆç¬¬ 53-113 è¡Œï¼‰

#### Kernel ç­¾å
```python
def _layer_norm_fwd_1pass_kernel(
    X,              # è¾“å…¥æŒ‡é’ˆ
    Y,              # è¾“å‡ºæŒ‡é’ˆ
    W,              # æƒé‡æŒ‡é’ˆ
    B,              # bias æŒ‡é’ˆ
    Z,              # gating æŒ‡é’ˆ (å¯é€‰)
    Mean,           # mean æŒ‡é’ˆ (ä»… LayerNormï¼ŒRMSNorm ä¸éœ€è¦)
    Rstd,           # 1/std æŒ‡é’ˆ
    stride_x_row,   # è¾“å…¥è¡Œæ­¥é•¿
    stride_y_row,   # è¾“å‡ºè¡Œæ­¥é•¿
    stride_z_row,   # z è¡Œæ­¥é•¿
    M,              # è¡Œæ•°
    N,              # åˆ—æ•° (group_size)
    eps,            # epsilon
    BLOCK_N: tl.constexpr,
    ...
):
```

#### Kernel æ‰§è¡Œé€»è¾‘

```python
# ç¬¬ 74-75 è¡Œï¼šè·å–å½“å‰å¤„ç†çš„è¡Œå’Œç»„
row = tl.program_id(0)     # 0 åˆ° M-1
group = tl.program_id(1)   # 0 åˆ° ngroups-1

# ç¬¬ 76-77 è¡Œï¼šè®¡ç®—è¾“å…¥å’Œè¾“å‡ºçš„åç§»
X += row * stride_x_row + group * N
Y += row * stride_y_row + group * N

# ç¬¬ 87-88 è¡Œï¼šåŠ è½½è¾“å…¥æ•°æ® (ä¸€ä¸ª group çš„æ•°æ®)
cols = tl.arange(0, BLOCK_N)
x = tl.load(X + cols, mask=cols < N, other=0.0)

# ç¬¬ 92-100 è¡Œï¼šè®¡ç®— mean å’Œ rstd
# ... (LayerNorm æˆ– RMSNorm çš„è®¡ç®—é€»è¾‘)

# ç¬¬ 102-108 è¡Œï¼šå½’ä¸€åŒ–å’Œçº¿æ€§å˜æ¢
w = tl.load(W + cols, mask=mask)
x_hat = (x - mean) * rstd  # æˆ– x * rstd (RMSNorm)
y = x_hat * w + b          # æˆ– x_hat * w (æ—  bias)

# ç¬¬ 109-111 è¡Œï¼šå¯é€‰çš„ gating
if HAS_Z and NORM_BEFORE_GATE:
    z = tl.load(Z + cols, mask=mask)
    y *= z * tl.sigmoid(z)  # Swish/SiLU gating

# ç¬¬ 113 è¡Œï¼šå­˜å‚¨è¾“å‡º
tl.store(Y + cols, y, mask=mask)
```

**å…³é”®è§‚å¯Ÿ**ï¼š
- æ¯ä¸ª `(row, group)` ç¨‹åºå¤„ç†è¾“å…¥çš„ä¸€ä¸ªç‰‡æ®µï¼ˆé•¿åº¦ä¸º `group_size`ï¼‰
- è¾“å‡ºå†™å…¥ä¸è¾“å…¥å¯¹åº”çš„ä½ç½®
- **è¾“å‡ºçš„å½¢çŠ¶å®Œå…¨ç”±è¾“å…¥å†³å®šï¼Œæ²¡æœ‰ç»´åº¦å˜åŒ–**

### ç¬¬ 5 æ­¥ï¼šè¿”å›ç»“æœï¼ˆç¬¬ 181 è¡Œï¼‰

```python
return out, mean, rstd
```

**è¿”å›å€¼**ï¼š
- `out`ï¼šå½¢çŠ¶ `[M, N]` - å½’ä¸€åŒ–åçš„è¾“å‡º
- `mean`ï¼šå½¢çŠ¶ `[ngroups * M]` - æ¯ä¸ªç»„çš„å‡å€¼ï¼ˆä»… LayerNormï¼‰
- `rstd`ï¼šå½¢çŠ¶ `[ngroups * M]` - æ¯ä¸ªç»„çš„ 1/std

### ç¬¬ 6 æ­¥ï¼šæ¢å¤åŸå§‹å½¢çŠ¶ï¼ˆç¬¬ 223 è¡Œï¼‰

```python
return y.reshape(x_shape_og)
```

**æœ€ç»ˆè¾“å‡º**ï¼š
- ä» 2D å½¢çŠ¶ `[M, N]` æ¢å¤åˆ°åŸå§‹å½¢çŠ¶ `x_shape_og`

---

## è¾“å‡ºå½¢çŠ¶æ€»ç»“

### ğŸ¯ æ ¸å¿ƒç»“è®º

**åœ¨ `_layer_norm_fwd` å‡½æ•°ä¸­ï¼Œè¾“å‡º `out` çš„å½¢çŠ¶ä¸º `[M, N]`**

**åœ¨ `LayerNormFn.forward` ä¸­ï¼Œæœ€ç»ˆè¾“å‡ºçš„å½¢çŠ¶ä¸º `x_shape_og`ï¼ˆåŸå§‹è¾“å…¥å½¢çŠ¶ï¼‰**

### å½¢çŠ¶å¯¹åº”å…³ç³»

| å±‚çº§ | è¾“å…¥å½¢çŠ¶ | è¾“å‡ºå½¢çŠ¶ | è¯´æ˜ |
|------|----------|----------|------|
| `LayerNormFn.forward` (è¾“å…¥) | `[d1, d2, ..., dk, N]` | - | åŸå§‹è¾“å…¥ï¼Œä»»æ„ç»´åº¦ |
| é‡å¡‘å | `[M, N]` | - | `M = d1 * d2 * ... * dk` |
| `_layer_norm_fwd` (è¾“å‡º) | - | `[M, N]` | ä¸è¾“å…¥ç›¸åŒ |
| `LayerNormFn.forward` (è¾“å‡º) | - | `[d1, d2, ..., dk, N]` | æ¢å¤åŸå§‹å½¢çŠ¶ |

### å…³é”®ç‚¹

1. **å½¢çŠ¶ä¿æŒä¸å˜**ï¼š`out.shape == x.shape`
2. **åœ¨ 2D å¤„ç†**ï¼šå†…éƒ¨å°†è¾“å…¥é‡å¡‘ä¸º `[M, N]` è¿›è¡Œå¤„ç†
3. **æ¢å¤åŸå§‹ç»´åº¦**ï¼šæœ€ç»ˆè¾“å‡ºæ¢å¤åˆ°åŸå§‹è¾“å…¥çš„å½¢çŠ¶
4. **ç‰¹å¾ç»´åº¦ä¸å˜**ï¼šæœ€åä¸€ä¸ªç»´åº¦ `N` å§‹ç»ˆä¿æŒä¸å˜

---

## å®é™…ç¤ºä¾‹

### ç¤ºä¾‹ 1ï¼šæ ‡å‡† LayerNormï¼ˆ3D è¾“å…¥ï¼‰

```python
# è¾“å…¥
x = torch.randn(8, 128, 768)  # [batch_size, seq_len, hidden_dim]
weight = torch.ones(768)
bias = torch.zeros(768)

# å†…éƒ¨å¤„ç†
M, N = 8 * 128, 768           # M=1024, N=768
x_2d = x.reshape(1024, 768)   # [M, N]

# Kernel æ‰§è¡Œ
grid = (1024, 1)               # (M, ngroups=1)
out_2d = torch.empty(1024, 768)  # [M, N]

# æœ€ç»ˆè¾“å‡º
out = out_2d.reshape(8, 128, 768)  # [batch_size, seq_len, hidden_dim]
```

**è¾“å‡ºå½¢çŠ¶**ï¼š`[8, 128, 768]` âœ… ä¸è¾“å…¥ç›¸åŒ

### ç¤ºä¾‹ 2ï¼šGroupNormï¼ˆ4D è¾“å…¥ï¼‰

```python
# è¾“å…¥
x = torch.randn(4, 64, 32, 512)  # [batch, height, width, channels]
weight = torch.ones(512)
bias = torch.zeros(512)
group_size = 128  # æ¯ç»„ 128 ä¸ªç‰¹å¾

# å†…éƒ¨å¤„ç†
M, N = 4 * 64 * 32, 512        # M=8192, N=512
ngroups = 512 // 128            # ngroups=4
x_2d = x.reshape(8192, 512)     # [M, N]

# Kernel æ‰§è¡Œ
grid = (8192, 4)                # (M, ngroups=4)
out_2d = torch.empty(8192, 512)  # [M, N]

# æœ€ç»ˆè¾“å‡º
out = out_2d.reshape(4, 64, 32, 512)  # [batch, height, width, channels]
```

**è¾“å‡ºå½¢çŠ¶**ï¼š`[4, 64, 32, 512]` âœ… ä¸è¾“å…¥ç›¸åŒ

### ç¤ºä¾‹ 3ï¼šRMSNorm with Gatingï¼ˆ2D è¾“å…¥ï¼‰

```python
# è¾“å…¥
x = torch.randn(1024, 2048)    # [tokens, hidden_dim]
z = torch.randn(1024, 2048)    # gating åˆ†æ”¯
weight = torch.ones(2048)
bias = None  # RMSNorm é€šå¸¸æ²¡æœ‰ bias

# å†…éƒ¨å¤„ç†
M, N = 1024, 2048               # å·²ç»æ˜¯ 2D
x_2d = x                        # æ— éœ€é‡å¡‘

# Kernel æ‰§è¡Œ
grid = (1024, 1)                # (M, ngroups=1)
out_2d = torch.empty(1024, 2048)  # [M, N]

# æœ€ç»ˆè¾“å‡º
out = out_2d                    # [tokens, hidden_dim]
```

**è¾“å‡ºå½¢çŠ¶**ï¼š`[1024, 2048]` âœ… ä¸è¾“å…¥ç›¸åŒ

---

## Grid é…ç½®è¯¦è§£

### Grid ç»´åº¦

```python
grid = (M, ngroups)
```

| Grid ç»´åº¦ | å¤§å° | å«ä¹‰ | ç¤ºä¾‹ |
|-----------|------|------|------|
| `grid[0]` | `M` | è¾“å…¥çš„è¡Œæ•° | `batch_size * seq_len` |
| `grid[1]` | `ngroups` | ç»„æ•° | `N // group_size` |

### ä¸åŒåœºæ™¯ä¸‹çš„ Grid

#### åœºæ™¯ 1ï¼šæ ‡å‡† LayerNorm
```python
group_size = N  # é»˜è®¤
ngroups = 1
grid = (M, 1)
# æ¯è¡Œä½œä¸ºä¸€ä¸ªæ•´ä½“è¿›è¡Œå½’ä¸€åŒ–
```

#### åœºæ™¯ 2ï¼šGroupNorm
```python
group_size = 128  # è‡ªå®šä¹‰
N = 512
ngroups = 4
grid = (M, 4)
# æ¯è¡Œåˆ†æˆ 4 ç»„ï¼Œæ¯ç»„ç‹¬ç«‹å½’ä¸€åŒ–
```

### Kernel ç¨‹åºåˆ†é…

```python
# Triton å¯åŠ¨ M * ngroups ä¸ªç¨‹åº
total_programs = M * ngroups

# æ¯ä¸ªç¨‹åºå¤„ç†:
# - ç¬¬ row è¡Œ (row = program_id(0))
# - ç¬¬ group ç»„ (group = program_id(1))
# - å…± group_size ä¸ªå…ƒç´ 
```

---

## å†…å­˜å¸ƒå±€åˆ†æ

### è¾“å…¥è¾“å‡ºæŒ‡é’ˆåç§»

```python
# ç¬¬ 76-77 è¡Œï¼šæŒ‡é’ˆåç§»è®¡ç®—
X += row * stride_x_row + group * N
Y += row * stride_y_row + group * N
```

**è¯´æ˜**ï¼š
- `row * stride_x_row`ï¼šè·³è½¬åˆ°ç¬¬ `row` è¡Œçš„èµ·å§‹ä½ç½®
- `group * N`ï¼šè·³è½¬åˆ°ç¬¬ `group` ç»„çš„èµ·å§‹ä½ç½®
- å¯¹äºè¿ç»­å­˜å‚¨çš„å¼ é‡ï¼Œ`stride_x_row = N`

### æ•°æ®è®¿é—®æ¨¡å¼

```
è¡Œ 0:  [g0: 0â†’127] [g1: 128â†’255] [g2: 256â†’383] [g3: 384â†’511]
è¡Œ 1:  [g0: 0â†’127] [g1: 128â†’255] [g2: 256â†’383] [g3: 384â†’511]
...
è¡Œ M-1:[g0: 0â†’127] [g1: 128â†’255] [g2: 256â†’383] [g3: 384â†’511]
```

æ¯ä¸ª `(row, group)` ç¨‹åºç‹¬ç«‹å¤„ç†ä¸€ä¸ªå—ã€‚

---

## å½¢çŠ¶å˜åŒ–æµç¨‹å›¾

```
ç”¨æˆ·è¾“å…¥:
  x.shape = [batch, seq_len, hidden_dim]
        â†“
ä¿å­˜åŸå§‹å½¢çŠ¶:
  x_shape_og = [batch, seq_len, hidden_dim]
        â†“
é‡å¡‘ä¸º 2D:
  x.shape = [M, N]
  M = batch * seq_len
  N = hidden_dim
        â†“
åˆ›å»ºè¾“å‡º:
  out = torch.empty_like(x)
  out.shape = [M, N]
        â†“
Triton Kernel æ‰§è¡Œ:
  grid = (M, ngroups)
  æ¯ä¸ªç¨‹åº: å¤„ç† [row, group*group_size : (group+1)*group_size]
        â†“
Kernel è¾“å‡º:
  out.shape = [M, N]  (å¡«å……å®Œæ¯•)
        â†“
æ¢å¤åŸå§‹å½¢çŠ¶:
  out.shape = [batch, seq_len, hidden_dim]
        â†“
è¿”å›ç”¨æˆ·:
  result.shape = [batch, seq_len, hidden_dim]
```

---

## ä¸å…¶ä»–å½’ä¸€åŒ–çš„å¯¹æ¯”

| å½’ä¸€åŒ–ç±»å‹ | è¾“å…¥å½¢çŠ¶ | è¾“å‡ºå½¢çŠ¶ | ç‰¹ç‚¹ |
|-----------|----------|----------|------|
| LayerNorm | `[..., N]` | `[..., N]` | å½¢çŠ¶ä¸å˜ |
| BatchNorm | `[B, C, ...]` | `[B, C, ...]` | å½¢çŠ¶ä¸å˜ |
| GroupNorm | `[..., N]` | `[..., N]` | å½¢çŠ¶ä¸å˜ |
| InstanceNorm | `[B, C, ...]` | `[B, C, ...]` | å½¢çŠ¶ä¸å˜ |

**å…±åŒç‚¹**ï¼šæ‰€æœ‰å½’ä¸€åŒ–æ“ä½œéƒ½ä¿æŒè¾“å…¥è¾“å‡ºå½¢çŠ¶ä¸€è‡´ã€‚

---

## ä»£ç éªŒè¯

### éªŒè¯è„šæœ¬

```python
import torch
from sglang.srt.layers.attention.fla.layernorm_gated import layernorm_fn

# æµ‹è¯•ä¸åŒå½¢çŠ¶çš„è¾“å…¥
test_cases = [
    (8, 128, 768),           # [batch, seq_len, hidden_dim]
    (4, 64, 32, 512),        # [batch, height, width, channels]
    (1024, 2048),            # [tokens, hidden_dim]
    (2, 3, 4, 5, 128),       # 5D tensor
]

for shape in test_cases:
    x = torch.randn(*shape, device='cuda')
    N = shape[-1]
    weight = torch.ones(N, device='cuda')
    bias = torch.zeros(N, device='cuda')
    
    # è°ƒç”¨ LayerNorm
    out = layernorm_fn(x, weight, bias)
    
    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    assert out.shape == x.shape, f"Shape mismatch: {out.shape} != {x.shape}"
    print(f"âœ“ Input: {x.shape} â†’ Output: {out.shape}")
```

### é¢„æœŸè¾“å‡º

```
âœ“ Input: torch.Size([8, 128, 768]) â†’ Output: torch.Size([8, 128, 768])
âœ“ Input: torch.Size([4, 64, 32, 512]) â†’ Output: torch.Size([4, 64, 32, 512])
âœ“ Input: torch.Size([1024, 2048]) â†’ Output: torch.Size([1024, 2048])
âœ“ Input: torch.Size([2, 3, 4, 5, 128]) â†’ Output: torch.Size([2, 3, 4, 5, 128])
```

---

## æ€»ç»“

### ğŸ¯ æ ¸å¿ƒç­”æ¡ˆ

**`_layer_norm_fwd_1pass_kernel` çš„è¾“å‡º `out` çš„å½¢çŠ¶ä¸º `[M, N]`**

å…¶ä¸­ï¼š
- `M`ï¼šæ‰€æœ‰å‰å¯¼ç»´åº¦çš„ä¹˜ç§¯ï¼ˆè¡Œæ•°ï¼‰
- `N`ï¼šæœ€åä¸€ä¸ªç»´åº¦ï¼ˆç‰¹å¾ç»´åº¦ï¼‰

**æœ€ç»ˆè¿”å›ç»™ç”¨æˆ·çš„è¾“å‡ºå½¢çŠ¶ä¸è¾“å…¥å½¢çŠ¶å®Œå…¨ä¸€è‡´ã€‚**

### å…³é”®è¦ç‚¹

1. âœ… **å½¢çŠ¶ä¸å˜æ€§**ï¼šè¾“å‡ºå½¢çŠ¶å§‹ç»ˆç­‰äºè¾“å…¥å½¢çŠ¶
2. âœ… **2D å¤„ç†**ï¼šå†…éƒ¨å°†å¤šç»´è¾“å…¥é‡å¡‘ä¸º 2D è¿›è¡Œé«˜æ•ˆå¤„ç†
3. âœ… **å½¢çŠ¶æ¢å¤**ï¼šå¤„ç†å®Œæˆåæ¢å¤åˆ°åŸå§‹è¾“å…¥å½¢çŠ¶
4. âœ… **ç‰¹å¾ç»´åº¦ä¿æŒ**ï¼šæœ€åä¸€ä¸ªç»´åº¦ï¼ˆç‰¹å¾ç»´åº¦ï¼‰ä¸å˜
5. âœ… **æ”¯æŒä»»æ„ç»´åº¦**ï¼šæ”¯æŒ 2Dã€3Dã€4D æˆ–æ›´é«˜ç»´åº¦çš„è¾“å…¥

### è®¾è®¡ä¼˜åŠ¿

- **é€šç”¨æ€§**ï¼šæ”¯æŒä»»æ„å½¢çŠ¶çš„è¾“å…¥å¼ é‡
- **é«˜æ•ˆæ€§**ï¼šå†…éƒ¨ç»Ÿä¸€ä¸º 2D å¤„ç†ï¼Œåˆ©ç”¨ Triton ä¼˜åŒ–
- **çµæ´»æ€§**ï¼šæ”¯æŒæ ‡å‡† LayerNorm å’Œ GroupNorm
- **å¯ç»„åˆæ€§**ï¼šæ”¯æŒå¯é€‰çš„ gating æœºåˆ¶ï¼ˆä¸ z åˆ†æ”¯ç»„åˆï¼‰

---

æœ€åæ›´æ–°: 2025-01-17

