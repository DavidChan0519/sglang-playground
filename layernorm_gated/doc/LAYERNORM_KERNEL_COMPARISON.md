# _layer_norm_fwd_1pass_kernel é‡å¤å®šä¹‰åˆ†æ

## ğŸ“ ä¸¤ä¸ªå®šä¹‰çš„ä½ç½®

1. **FLA ç‰ˆæœ¬**ï¼š`python/sglang/srt/layers/attention/fla/layernorm_gated.py`
2. **Mamba ç‰ˆæœ¬**ï¼š`python/sglang/srt/layers/attention/mamba/ops/layernorm_gated.py`

---

## ğŸ” è¯¦ç»†å¯¹æ¯”

### 1. Triton Kernel å¯¹æ¯”

#### FLA ç‰ˆæœ¬ï¼ˆç¬¬ 53-113 è¡Œï¼‰
```python
@triton.heuristics({"HAS_BIAS": lambda args: args["B"] is not None})
@triton.heuristics({"HAS_Z": lambda args: args["Z"] is not None})
@triton.jit
def _layer_norm_fwd_1pass_kernel(
    X,  # pointer to the input
    Y,  # pointer to the output
    W,  # pointer to the weights
    B,  # pointer to the biases
    Z,  # pointer to the other branch
    Mean,  # pointer to the mean
    Rstd,  # pointer to the 1/std
    stride_x_row,  # âš ï¸ æ— ç±»å‹æ³¨è§£
    stride_y_row,
    stride_z_row,
    M,  # âš ï¸ æ— ç±»å‹æ³¨è§£
    N,  # âš ï¸ æ— ç±»å‹æ³¨è§£
    eps,
    BLOCK_N: tl.constexpr,
    HAS_BIAS: tl.constexpr,
    HAS_Z: tl.constexpr,
    NORM_BEFORE_GATE: tl.constexpr,
    IS_RMS_NORM: tl.constexpr,
):
```

#### Mamba ç‰ˆæœ¬ï¼ˆç¬¬ 14-74 è¡Œï¼‰
```python
@triton.heuristics({"HAS_BIAS": lambda args: args["B"] is not None})
@triton.heuristics({"HAS_Z": lambda args: args["Z"] is not None})
@triton.jit
def _layer_norm_fwd_1pass_kernel(
    X,
    Y,
    W,
    B,
    Z,
    Mean,
    Rstd,
    stride_x_row: tl.int64,  # âœ… æœ‰ç±»å‹æ³¨è§£
    stride_y_row: tl.int64,  # âœ… æœ‰ç±»å‹æ³¨è§£
    stride_z_row: tl.int64,  # âœ… æœ‰ç±»å‹æ³¨è§£
    M: tl.int64,  # âœ… æœ‰ç±»å‹æ³¨è§£
    N: tl.int64,  # âœ… æœ‰ç±»å‹æ³¨è§£
    eps,
    BLOCK_N: tl.constexpr,
    HAS_BIAS: tl.constexpr,
    HAS_Z: tl.constexpr,
    NORM_BEFORE_GATE: tl.constexpr,
    IS_RMS_NORM: tl.constexpr,
):
```

### ğŸ“Š Kernel å®ç°å·®å¼‚

| æ–¹é¢ | FLA ç‰ˆæœ¬ | Mamba ç‰ˆæœ¬ | æ˜¯å¦ç›¸åŒ |
|------|----------|------------|----------|
| **å‚æ•°ç±»å‹æ³¨è§£** | æ—  `tl.int64` | æœ‰ `tl.int64` | âŒ ä¸åŒ |
| **Kernel é€»è¾‘** | ç¬¬ 74-113 è¡Œ | ç¬¬ 35-74 è¡Œ | âœ… **å®Œå…¨ç›¸åŒ** |
| **è®¡ç®—æµç¨‹** | mean â†’ var â†’ rstd â†’ normalize | mean â†’ var â†’ rstd â†’ normalize | âœ… ç›¸åŒ |
| **Gating æ”¯æŒ** | `z * tl.sigmoid(z)` | `z * tl.sigmoid(z)` | âœ… ç›¸åŒ |
| **RMSNorm æ”¯æŒ** | `IS_RMS_NORM` åˆ†æ”¯ | `IS_RMS_NORM` åˆ†æ”¯ | âœ… ç›¸åŒ |
| **GroupNorm æ”¯æŒ** | `group` å‚æ•° | `group` å‚æ•° | âœ… ç›¸åŒ |

**ç»“è®º**ï¼š**Kernel æ ¸å¿ƒé€»è¾‘ 100% ç›¸åŒï¼Œä»…å‚æ•°ç±»å‹æ³¨è§£ä¸åŒ**

---

### 2. Python åŒ…è£…å±‚å¯¹æ¯”

#### FLA ç‰ˆæœ¬åŠŸèƒ½ï¼ˆ327 è¡Œï¼‰

**å®Œæ•´çš„ API å±‚æ¬¡**ï¼š

1. **Kernel å±‚**ï¼ˆç¬¬ 53-113 è¡Œï¼‰ï¼š
   ```python
   @triton.jit
   def _layer_norm_fwd_1pass_kernel(...)
   ```

2. **åº•å±‚å‡½æ•°**ï¼ˆç¬¬ 116-181 è¡Œï¼‰ï¼š
   ```python
   def _layer_norm_fwd(x, weight, bias, eps, z=None, out=None, 
                       group_size=None, norm_before_gate=True, is_rms_norm=False)
   ```
   - æ”¯æŒ `group_size`ï¼ˆGroupNormï¼‰
   - ä½¿ç”¨ `torch.get_device_module(x.device).device(x.device.index)` ä¸Šä¸‹æ–‡

3. **Autograd å‡½æ•°**ï¼ˆç¬¬ 184-223 è¡Œï¼‰ï¼š
   ```python
   class LayerNormFn(torch.autograd.Function):
       @staticmethod
       def forward(ctx, x, weight, bias, z=None, eps=1e-6, 
                   group_size=None, norm_before_gate=True, is_rms_norm=False)
       # æ”¯æŒåå‘ä¼ æ’­
   ```

4. **ç”¨æˆ·æ¥å£å‡½æ•°**ï¼ˆç¬¬ 226-246 è¡Œï¼‰ï¼š
   ```python
   def layernorm_fn(...)  # é€šç”¨ LayerNorm
   def rmsnorm_fn(...)    # RMSNorm
   ```

5. **nn.Module å°è£…**ï¼ˆç¬¬ 249-326 è¡Œï¼‰ï¼š
   ```python
   class LayerNorm(torch.nn.Module):  # æ ‡å‡† LayerNorm æ¨¡å—
   class RMSNorm(torch.nn.Module):     # RMSNorm æ¨¡å—
   ```

6. **å‚è€ƒå®ç°**ï¼ˆç¬¬ 17-47 è¡Œï¼‰ï¼š
   ```python
   def rms_norm_ref(...)  # CPU å‚è€ƒå®ç°ï¼Œç”¨äºéªŒè¯
   ```

#### Mamba ç‰ˆæœ¬åŠŸèƒ½ï¼ˆ173 è¡Œï¼‰

**ç®€åŒ–çš„ API**ï¼š

1. **Kernel å±‚**ï¼ˆç¬¬ 14-74 è¡Œï¼‰ï¼š
   ```python
   @triton.jit
   def _layer_norm_fwd_1pass_kernel(...)
   ```

2. **åº•å±‚å‡½æ•°**ï¼ˆç¬¬ 77-142 è¡Œï¼‰ï¼š
   ```python
   def _layer_norm_fwd(x, weight, bias, eps, z=None, out=None,
                       group_size=None, norm_before_gate=True, is_rms_norm=False)
   ```
   - ä½¿ç”¨ `torch.cuda.device(x.device.index)` ä¸Šä¸‹æ–‡

3. **ç”¨æˆ·æ¥å£å‡½æ•°**ï¼ˆç¬¬ 145-172 è¡Œï¼‰ï¼š
   ```python
   def rms_norm_gated(...)  # ä»…æä¾› RMSNorm æ¥å£
   ```
   - âŒ **æ²¡æœ‰ Autograd æ”¯æŒ**
   - âŒ **æ²¡æœ‰ nn.Module å°è£…**
   - âŒ **æ²¡æœ‰é€šç”¨ LayerNorm æ¥å£**
   - âŒ **æ²¡æœ‰å‚è€ƒå®ç°**

### ğŸ“Š API åŠŸèƒ½å¯¹æ¯”

| åŠŸèƒ½ | FLA ç‰ˆæœ¬ | Mamba ç‰ˆæœ¬ | è¯´æ˜ |
|------|----------|------------|------|
| **Triton Kernel** | âœ… | âœ… | æ ¸å¿ƒé€»è¾‘ç›¸åŒ |
| **ç±»å‹æ³¨è§£** | âŒ | âœ… | Mamba ç‰ˆæœ¬æ›´ä¸¥æ ¼ |
| **Autograd æ”¯æŒ** | âœ… `LayerNormFn` | âŒ | FLA æ”¯æŒåå‘ä¼ æ’­ |
| **nn.Module** | âœ… `LayerNorm`, `RMSNorm` | âŒ | FLA æä¾›æ¨¡å—å°è£… |
| **LayerNorm æ¥å£** | âœ… `layernorm_fn` | âŒ | FLA åŠŸèƒ½æ›´å…¨ |
| **RMSNorm æ¥å£** | âœ… `rmsnorm_fn` | âœ… `rms_norm_gated` | éƒ½æ”¯æŒ |
| **å‚è€ƒå®ç°** | âœ… `rms_norm_ref` | âŒ | FLA æä¾›éªŒè¯å‚è€ƒ |
| **è®¾å¤‡ä¸Šä¸‹æ–‡** | `torch.get_device_module()` | `torch.cuda.device()` | ç•¥æœ‰ä¸åŒ |
| **ä»£ç è¡Œæ•°** | 327 è¡Œ | 173 è¡Œ | FLA åŠŸèƒ½æ›´ä¸°å¯Œ |

---

## ğŸ¤” ä¸ºä»€ä¹ˆå­˜åœ¨ä¸¤ä»½ï¼Ÿ

### 1. æ¥æºä¸åŒ

#### FLA ç‰ˆæœ¬
```python
# Adapt from https://github.com/fla-org/flash-linear-attention/blob/main/fla/modules/layernorm_gated.py
# Copyright (c) 2024, Tri Dao.
# Based on the Triton LayerNorm tutorial
```
- æ¥è‡ª **Flash Linear Attention (FLA)** é¡¹ç›®
- ç”± Tri Daoï¼ˆFlashAttention ä½œè€…ï¼‰å¼€å‘
- è®¾è®¡ç”¨äº **çº¿æ€§æ³¨æ„åŠ›** å’Œ **é—¨æ§æœºåˆ¶**

#### Mamba ç‰ˆæœ¬
```python
# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
# Copyright (c) 2024, Tri Dao.
# Adapted from https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/triton/layernorm_gated.py
```
- æ¥è‡ª **Mamba (State Space Models)** é¡¹ç›®
- ä¹Ÿç”± Tri Dao å¼€å‘
- é€šè¿‡ vLLM é¡¹ç›®å¼•å…¥
- è®¾è®¡ç”¨äº **Mamba æ¶æ„**

### 2. ä½¿ç”¨åœºæ™¯ä¸åŒ

#### FLA ç‰ˆæœ¬çš„ä½¿ç”¨åœºæ™¯
```python
# åœ¨ FLA å­ç›®å½•ä¸­çš„æ–‡ä»¶ä½¿ç”¨
python/sglang/srt/layers/attention/fla/
â”œâ”€â”€ layernorm_gated.py          # â† å®šä¹‰å¤„
â”œâ”€â”€ chunk.py                     # å¯èƒ½ä½¿ç”¨
â”œâ”€â”€ fused_recurrent.py          # å¯èƒ½ä½¿ç”¨
â””â”€â”€ fused_sigmoid_gating_recurrent.py  # å¯èƒ½ä½¿ç”¨
```

**ç”¨é€”**ï¼š
- æ”¯æŒ **Flash Linear Attention** æ¨¡å‹
- ç”¨äº **Gated Delta Rule** ç­‰çº¿æ€§æ³¨æ„åŠ›æœºåˆ¶
- éœ€è¦å®Œæ•´çš„ Autograd å’Œ nn.Module æ”¯æŒ

#### Mamba ç‰ˆæœ¬çš„ä½¿ç”¨åœºæ™¯
```python
# åœ¨ Mamba å­ç›®å½•ä¸­çš„æ–‡ä»¶ä½¿ç”¨
python/sglang/srt/layers/attention/mamba/
â”œâ”€â”€ ops/
â”‚   â””â”€â”€ layernorm_gated.py      # â† å®šä¹‰å¤„
â”œâ”€â”€ mamba.py                     # ä½¿ç”¨
â””â”€â”€ causal_conv1d_triton.py     # å¯èƒ½ä½¿ç”¨
```

**ç”¨é€”**ï¼š
- æ”¯æŒ **Mamba/Mamba2** æ¨¡å‹
- ç”¨äº **SSM (State Space Models)**
- ä»…éœ€è¦ç®€å•çš„å‰å‘è®¡ç®—ï¼ˆæ¨ç†æ—¶ï¼‰

### 3. æ¶æ„éš”ç¦»åŸåˆ™

```
python/sglang/srt/layers/attention/
â”œâ”€â”€ fla/                    # Flash Linear Attention ç”Ÿæ€
â”‚   â””â”€â”€ layernorm_gated.py  # â† FLA ä¸“ç”¨
â”‚
â””â”€â”€ mamba/                  # Mamba SSM ç”Ÿæ€
    â””â”€â”€ ops/
        â””â”€â”€ layernorm_gated.py  # â† Mamba ä¸“ç”¨
```

**è®¾è®¡ç†å¿µ**ï¼š
1. **æ¨¡å—ç‹¬ç«‹æ€§**ï¼šFLA å’Œ Mamba æ˜¯ä¸¤ä¸ªç‹¬ç«‹çš„æ¨¡å‹æ¶æ„
2. **ä¾èµ–éš”ç¦»**ï¼šé¿å…è·¨æ¨¡å—ä¾èµ–
3. **ç»´æŠ¤ä¾¿åˆ©**ï¼šå¯ä»¥ç‹¬ç«‹æ›´æ–°å„è‡ªçš„å®ç°
4. **ä»£ç æº¯æº**ï¼šä¿ç•™åŸå§‹é¡¹ç›®çš„ä»£ç ç»“æ„

---

## ğŸ’¡ æ˜¯å¦çœŸçš„å†—ä½™ï¼Ÿ

### âœ… ä»ä»£ç å¤ç”¨è§’åº¦çœ‹ï¼šæ˜¯å†—ä½™çš„

**ç†ç”±**ï¼š
1. Kernel æ ¸å¿ƒé€»è¾‘ 100% ç›¸åŒ
2. å¯ä»¥æå–åˆ°å…¬å…±æ¨¡å—

**æ”¹è¿›å»ºè®®**ï¼š
```python
# æ–¹æ¡ˆ 1ï¼šæå–å…¬å…± kernel
# python/sglang/srt/layers/kernels/layernorm_gated.py
@triton.jit
def _layer_norm_fwd_1pass_kernel(...):
    # å…¬å…±å®ç°

# FLA å’Œ Mamba åˆ†åˆ«å¯¼å…¥
from sglang.srt.layers.kernels.layernorm_gated import _layer_norm_fwd_1pass_kernel
```

### âŒ ä»å·¥ç¨‹å®è·µè§’åº¦çœ‹ï¼šä¸å®Œå…¨å†—ä½™

**ç†ç”±**ï¼š

1. **åŠŸèƒ½å·®å¼‚æ˜æ˜¾**ï¼š
   - FLA ç‰ˆæœ¬ï¼š327 è¡Œï¼Œå®Œæ•´çš„ APIï¼ˆAutograd + nn.Moduleï¼‰
   - Mamba ç‰ˆæœ¬ï¼š173 è¡Œï¼Œä»…æ¨ç†æ¥å£

2. **ç»´æŠ¤ç‹¬ç«‹æ€§**ï¼š
   - ä¸¤ä¸ªæ¥æºé¡¹ç›®ï¼ˆFLA-org å’Œ state-spacesï¼‰å¯èƒ½ç‹¬ç«‹æ¼”è¿›
   - ä¿æŒå„è‡ªçš„æ›´æ–°è·¯å¾„

3. **ä¾èµ–éš”ç¦»**ï¼š
   - ä½¿ç”¨ FLA ä¸éœ€è¦ Mamba çš„ä»£ç 
   - ä½¿ç”¨ Mamba ä¸éœ€è¦ FLA çš„ Autograd é€»è¾‘

4. **æ€§èƒ½è°ƒä¼˜ç©ºé—´**ï¼š
   - FLA ç‰ˆæœ¬å¯èƒ½é’ˆå¯¹çº¿æ€§æ³¨æ„åŠ›ä¼˜åŒ–
   - Mamba ç‰ˆæœ¬å¯èƒ½é’ˆå¯¹ SSM ä¼˜åŒ–
   - ç±»å‹æ³¨è§£å·®å¼‚å¯èƒ½å½±å“ Triton ç¼–è¯‘

---

## ğŸ”§ å®é™…åŒºåˆ«æ€»ç»“

### Kernel å±‚é¢

| åŒºåˆ«é¡¹ | å½±å“ |
|--------|------|
| **ç±»å‹æ³¨è§£** | Mamba ç‰ˆæœ¬çš„ `tl.int64` æ³¨è§£å¯èƒ½è®© Triton ç¼–è¯‘å™¨ç”Ÿæˆæ›´ä¼˜åŒ–çš„ä»£ç  |
| **æ ¸å¿ƒé€»è¾‘** | å®Œå…¨ç›¸åŒ |

### åŒ…è£…å±‚é¢

| å±‚é¢ | FLA ç‰ˆæœ¬ | Mamba ç‰ˆæœ¬ |
|------|----------|------------|
| **è®­ç»ƒæ”¯æŒ** | âœ… å®Œæ•´ Autograd | âŒ ä»…æ¨ç† |
| **API ä¸°å¯Œåº¦** | âœ… 6 ä¸ªæ¥å£ | âš ï¸ 1 ä¸ªæ¥å£ |
| **æ˜“ç”¨æ€§** | âœ… nn.Module å°è£… | âš ï¸ éœ€è¦æ‰‹åŠ¨è°ƒç”¨ |
| **ä»£ç å¤æ‚åº¦** | âš ï¸ 327 è¡Œ | âœ… 173 è¡Œç®€æ´ |

---

## ğŸ“ å»ºè®®

### çŸ­æœŸï¼ˆä¿æŒç°çŠ¶ï¼‰
âœ… **æ¨èä¿æŒä¸¤ä»½ç‹¬ç«‹å®ç°**

**åŸå› **ï¼š
1. åŠŸèƒ½å·®å¼‚å¤§ï¼ˆAutograd vs ä»…æ¨ç†ï¼‰
2. ç»´æŠ¤æˆæœ¬ä½ï¼ˆæ–‡ä»¶ä¸å¤§ï¼Œé€»è¾‘ç¨³å®šï¼‰
3. ä¾èµ–éš”ç¦»æ¸…æ™°

### ä¸­æœŸï¼ˆéƒ¨åˆ†åˆå¹¶ï¼‰
å¦‚æœæœªæ¥éœ€è¦ä¼˜åŒ–ï¼š

```python
# æ­¥éª¤ 1: æå–å…¬å…± kernel
# sglang/srt/layers/kernels/layernorm_triton.py
@triton.jit
def layernorm_fwd_1pass_kernel(
    X, Y, W, B, Z, Mean, Rstd,
    stride_x_row: tl.int64,  # ä½¿ç”¨ Mamba ç‰ˆæœ¬çš„ç±»å‹æ³¨è§£
    stride_y_row: tl.int64,
    stride_z_row: tl.int64,
    M: tl.int64,
    N: tl.int64,
    eps,
    BLOCK_N: tl.constexpr,
    HAS_BIAS: tl.constexpr,
    HAS_Z: tl.constexpr,
    NORM_BEFORE_GATE: tl.constexpr,
    IS_RMS_NORM: tl.constexpr,
):
    # ç»Ÿä¸€å®ç°
    ...

# æ­¥éª¤ 2: FLA å’Œ Mamba å„è‡ªä¿ç•™åŒ…è£…å±‚
# fla/layernorm_gated.py
from sglang.srt.layers.kernels.layernorm_triton import layernorm_fwd_1pass_kernel

class LayerNormFn(torch.autograd.Function):
    # FLA ä¸“ç”¨çš„ Autograd å®ç°
    ...

# mamba/ops/layernorm_gated.py
from sglang.srt.layers.kernels.layernorm_triton import layernorm_fwd_1pass_kernel

def rms_norm_gated(...):
    # Mamba ä¸“ç”¨çš„ç®€åŒ–æ¥å£
    ...
```

### é•¿æœŸï¼ˆå®Œå…¨ç»Ÿä¸€ï¼‰
å¦‚æœ FLA å’Œ Mamba éƒ½éœ€è¦å®Œæ•´åŠŸèƒ½ï¼š

```python
# sglang/srt/layers/layernorm/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ kernels.py          # å…¬å…± Triton kernel
â”œâ”€â”€ functional.py       # å‡½æ•°å¼æ¥å£
â”œâ”€â”€ modules.py          # nn.Module å°è£…
â””â”€â”€ autograd.py         # Autograd æ”¯æŒ
```

---

## ğŸ¯ æœ€ç»ˆç»“è®º

### 1. **Kernel å±‚é¢**ï¼šâœ… å®Œå…¨ç›¸åŒï¼ˆé™¤ç±»å‹æ³¨è§£ï¼‰
   - æ ¸å¿ƒè®¡ç®—é€»è¾‘ 100% ä¸€è‡´
   - ä»…ç±»å‹æ³¨è§£ä¸åŒï¼ˆMamba æ›´ä¸¥æ ¼ï¼‰

### 2. **API å±‚é¢**ï¼šâŒ åŠŸèƒ½å·®å¼‚å¤§
   - FLAï¼šå®Œæ•´æ¡†æ¶ï¼ˆ327 è¡Œï¼Œæ”¯æŒè®­ç»ƒï¼‰
   - Mambaï¼šç®€åŒ–æ¥å£ï¼ˆ173 è¡Œï¼Œä»…æ¨ç†ï¼‰

### 3. **æ˜¯å¦å†—ä½™**ï¼šâš ï¸ éƒ¨åˆ†å†—ä½™ï¼Œä½†æœ‰åˆç†æ€§
   - **å†—ä½™éƒ¨åˆ†**ï¼šKernel æ ¸å¿ƒé€»è¾‘
   - **ä¸å†—ä½™éƒ¨åˆ†**ï¼šåŒ…è£…å±‚ã€API è®¾è®¡ã€ä½¿ç”¨åœºæ™¯

### 4. **å»ºè®®**ï¼š
   - **ç°çŠ¶å¯æ¥å—**ï¼šç»´æŠ¤æˆæœ¬ä½ï¼Œæ¶æ„æ¸…æ™°
   - **å¯ä¼˜åŒ–ç‚¹**ï¼šæœªæ¥å¯è€ƒè™‘æå–å…¬å…± kernel
   - **ä¸å»ºè®®å¼ºåˆ¶åˆå¹¶**ï¼šä¼šç ´åæ¨¡å—ç‹¬ç«‹æ€§

---

**åˆ›å»ºæ—¶é—´**ï¼š2025-01-17  
**åˆ†æå¯¹è±¡**ï¼š
- `python/sglang/srt/layers/attention/fla/layernorm_gated.py`
- `python/sglang/srt/layers/attention/mamba/ops/layernorm_gated.py`

